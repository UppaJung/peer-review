# What <u>You</u> Can Do to Fix Peer Review

Scoring other's research work does not advance science. Scoring does not help authors improve their work. Scoring does not help the work's audience understand the work, its limitations, or whether its conclusions are credible.

But scoring does undermine our objectivity, amplifying our implicit biases: we are predisposed to appreciate works that are familiar in approach, language, and style to our own work; we trust results more easily when they confirm our existing beliefs and hopes. We cannot quantify how much a work deserves to be trusted, recognized, or otherwise valued, without being influenced by our biases. We also cannot stop these biases from propagating from our scores to our written feedback. We may try to be objective, such as by ensuring every word we write is backed by facts, but we can only report a small fraction of the facts we observe. Facts that confirm the fairness of our scoring intentions are likely to appear more salient than facts that contradict our fairness.

Despite the hazards of scoring, the principal function of the most widely-available form of peer review, *publication review*, is to assign each work a score of *accept* or *reject*. Different journals and conferences may use different criteria to assign these scores, but they must ultimately choose between publishing a work or refusing to do so.[^funding]

The scant evidence available certainly does not suggest that we assign *accepts* and *rejects* objectively. In experiments by the NeurIPS conference in 2014 and again in 2021, half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] It's fashionable to refer to this inconsistency as *randomness*, but doing so promotes the belief that our reviewing errors are unbiased and that their impact diminishes over the span of a career. Biases grow worse over time to become systemic. For example, a bias against underrepresented researchers, or against those willing to challenge popular ideas, would reduce the likelihood that young scientists in these categories to survive long enough in their careers to become reviewers themselves.[^selection-of-scientists]

Scoring is surely not the only source of bias when we review others' work, but it is a source we can eliminate. We need not worry that we will bias our feedback to justify our scores if we do not produce scores. Removing a source of bias can make peer review objective, equitable, and just.

Moreover, *scoreless* peer review can serve the two *scientific* functions we ascribe to peer review: to help the work's authors improve their research work, and to help the audience of a research work understand it and evaluate its credibility.[^attention] I will refer to these functions as *author-assistive* and *audience-assistive* review.

### Author-assistive review

When we scrutinize the work of students and colleagues to catch errors, offer clarifications, and suggest improvements, we are informally conducting *author-assistive* peer review. We rarely provide scores when performing this function, even if doing so in preparation for publication review.

Alas, the social norm of offering author-assistive review only to those close to us, and reviewing everyone else's work primarily through publication review, has unintended harms. A field's insiders can receive constructive feedback quickly by asking colleagues. Those whose only option is publication review may wait months longer for assistance, only to receive feedback that is written to justify a rejection.[^mostly-rejections]

If we want to ensure our well-intentioned service to others does unwittingly perpetuate the disadvantages faced by underrepresented groups and other outsiders, we should each make ourselves at least as accessible for author-assistive peer review as we are for publication review.[^pre-experimental-review] Put more simply, we should be as available to help others as to judge them. (Getting started is easy. See "[What You Can Do <u>Right Now</u>](#rightnow)" below.)

### Audience-assistive review

We can also conduct scoreless peer review to help audiences understand research work and evaluate its credibility.[^subjective-credibility] And we should, because publication review is poorly suited to this function. Publications' acceptance criteria include factors such as ‘novelty’ and ‘significance’ that are not only unrelated to credibility, but subvert it by rewarding unscientific practices and chance results.[^evaluating-experiments-by-their-outcomes] When acceptances are misconstrued as endorsements readers overlook key limitations. Publication review rarely detects outright fraud, but it often legitimizes fraudulent work.[^rarely-detects-fraud] The confidentiality of feedback from publication review hides reviewers' biases, allowing them to fester.[^open-peer-review] And publication review slows the pace of scientific discourse because reviewers must agree on a final outcome, often requiring them to review others works against which the work will compete. Publication review adds further delays by rejecting credible work for such reasons ‘novelty’ and ‘significance’ needed to maintain the perception of exclusivity and prestige.


*Audience-assistive peer review* enlists reviewers to provide feedback addressed to the work's audience to help them understand the work and any issues that might impact its credibility.[^information-asymmetries] We can provide nuanced feedback to help audiences differentiate extremely meticulous findings from those that are somewhat credible, those that are glaringly wrong, and everything in between.[^rejects-invisible-if-unpublished] Our feedback can help audiences develop the skills to identify strengths and weaknesses of research results themselves. We can provide feedback quickly, without comparing the work to others' work and without coordinate with others to choose an outcome. And while biases may still affect the contents of our reviews, sharing that content with the work's audience exposes those biases for others to identify, refute, and even study.

When used as a substitute for publication review, audience-assistive review can also free authors to write for their audience, who will ultimately judge our work, rather than to appeal to reviewers.[^written-for-reviewers]

We should each make ourselves at least as accessible for *audience-assistive* peer review as we are for publication review. Put more simply, we should be as available to help the audience of a work understand it as we are to decide whether they will find it in an exclusive publication or on a pre-print server.

<a id="rightnow"></a>
### What You Can Do <u>Right Now</u> to Advance Scoreless Review

We each have a limited precious time to dedicate to service, and we each have time decide how much time to spend scoring others and how much time to spend assisting others.[^time] If you believe you should make ourselves at least as accessible for scoreless peer review as you are for publication review, you can take action right now. You don't need to wait for anyone else to agree. You don't need to wait for others to create and invite you to scoreless peer review panels.

**Just say that you are "available to conduct scoreless peer review"** on your webpage, blog, or social media profile. You can prefix or follow the above phrase with conditionals like that you "*try to be* available to conduct scoreless per review *when possible*", but use "available to conduct scoreless peer review" to be discoverable via a web search (e.g. via [Google](https://www.google.com/search?q=%22available+to+conduct+scoreless+peer+review%22) or [Bing](https://www.bing.com/search?q=%22available+to+conduct+scoreless+peer+review%22)).[^author-only]  You can add conditions, such as which topic areas you feel qualified to review, which you are interested in reviewing, and which you do not want to review.[^fees] (I do so [here](https://www.stuartschechter.org/about#scoreless-peer-review).)

Signalling your availability should not only be easy to do, but it should also be easy to undo if you change your mind. And it will have immediate impact. Authors who may not otherwise have access to assistive review will now be able to find you. More organized forms of scoreless peer review panels can form once enough of us act individually.

[^time]: If you are worried about the time required, consider that scoreless peer review could actually reduce all of our workloads. As more work is submitted for publication having already benefited from scoreless review, with its credibility already evaluated, less of that work may need to be rejected. Consider that work submitted to review committees with acceptance rates of 25% will require an expectation of four submissions to be published, consuming the time of four sets of reviewers. Increasing the probability of acceptance to 34% reduces the expected number of submissions by one (from 4 to 3), removing the expected review workload by an entire set of reviewers. Further, if publication review is no longer needed to establish the credibility of work, more conferences could forgo using peer reviewers to choose which works they think attendees will want to see and, instead, ask prospective attendees which works they actually want to see.

### Scoreless Peer Review Can Help Fix Publication Review

Scoreless peer review alone cannot fix all that is broken with the industry of science and its reliance on publication review.

While the *endeavor of science* may be a collective search for truth, the *industry of science* that has grown around this endeavor is fiercely competitive and its institutions serve their own interests. Journals and conferences compete for prestige to garner subscriptions and attendance. Research institutions compete for prestige and to hire talent. Publication review has become a tool to allocate prestige within this system.

Most of us have no choice but to submit our work for publication review because we, or are co-authors, must publish in the "right places" to survive in this industry. Participating in this system can make it easy to conflate service to the industry of science with service to the endeavor of science. This can lead us to believe that rejecting the great majority of others' scientific research is service to science.

What scoreless peer can do is to isolate service to the endeavor of science from service to the industry of science. As authors, it can empower us to seek help improving our work, and assistance vetting our work, without being ranked for recognition and prestige. As reviewers, scoreless peer review presents the opportunity to help our peers improve their, and their audience understand it, without exercising power over whether the work receives recognition and prestige. It can help undermine the premise that exercising such power is service to the endeavor of science, or that the pursuit of such power is even compatible with the endeavor of scienceå.

---

Stuart Schechter ([@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist)) wrote this with suggestions and feedback from  [Joseph Bonneau](https://jbonneau.com/), [Sauvik Das](https://www.hcii.cmu.edu/people/sauvik-das), [Mary L. Gray](https://marylgray.org/), [Cormac Herley](https://cormac.herley.org/), [Jon Howell](https://research.vmware.com/researchers/jon-howell), [Michael D. Smith](https://seas.harvard.edu/person/michael-smith), and [Erin Walk](https://erinwalk.org/).

[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).

[^consistency]: [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/) followed the earlier experiment at [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS). The 2014 call for papers stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity” whereas the 2021 call for papers listed no evaluation criteria.

[^mostly-rejections]: Since journals and conferences compete to be exclusive, those submitting to the publications with the most experts can expect the highest rejection rates.

[^pre-experimental-review]: We should also offer to assist authors *before* they conduct experiments, helping them refine their hypotheses and improve their experimental designs prior to incurring the risks, costs, and time the experiments require. The current practice of reviewing and rejecting experimental designs at publication time, after the costs of conducting the experiment have been sunk, is wasteful and frustrating for all involved. Pre-experimental review and registration of experimental designs can increase the chance that null results will be published. A [2018 examination](https://psyarxiv.com/3czyt) of 127 bio-medical and psychological science registered reports (pre-registered studies) showed a 61% null result rate, as opposed to a typical 5-20% null result rate for studies published in venues that did not require pre-registration.

[^subjective-credibility]: It's tempting to assume that we could accept or reject experiments objectively if we examined only their credibility. To understand why we cannot, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the credibility of the experiment and it must be accepted. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well, and must reject the study.

[^open-peer-review]: There are forms of publication review that open up some of their discourse, such as [open peer review](https://en.wikipedia.org/wiki/Open_peer_review), in which the reviews of accepted papers become public, but biases that cause papers to be rejected still remain hidden.

<!-- [^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so. -->

[^rejects-invisible-if-unpublished]: Further, when rejected work goes unpublished they disappear from scientific discourse, we can't learn from any mistakes that were made, and so the same mistakes and failed experiments can be re-run over and over. We bias research in favor of chance results.

[^written-for-reviewers]: Research works today are often more written for reviewers than a work's true audience, at a cost both to that audience and to authors. For example, reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the number of citations to grow out of hand. Bibliographies have evolved like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.


[^anonymity]: The corrupting influence of power to score others' work can be compounded when we exercise that power anonymously. Audience-assistive peer review can be performed anonymously, but there may be less reason to when reviews do not produce scores or outcomes.

[^selection-of-scientists]: In [The natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384), Smaldino and McElreath argue that  that publication review also favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. 

[^information-asymmetries]: Whereas publication review can reinforce knowledge asymmetries, audience-assistive feedback is designed to reduce knowledge asymmetries, reducing the knowledge gap between authors and audience.
 
[^author-only]: If you are not willing to perform both author- and audience-assistive peer review, I'd suggest "available to conduct author-assistive peer review" or "available to conduct audience-assistive peer review".

[^fees]: Peer review as unpaid (volunteer) service has been a norm in much of the scientific community. Yet, author-assistive reviews are often performed by colleagues at the same employer, or collaborators who are on the same grants, and so are often implicitly paid. We need to be understanding that reviewers may reasonably expect to be paid when reviewing industrial research and research by authors who are as well compensated, if not better compensated, than the reviewer is.

[^rarely-detects-fraud]: The failure of peer review to detect fraud by Adam Mastroianni in the postmortem of [The rise and fall of peer review](https://www.experimental-history.com/i/90286657/postmortem), starting at paragraph 4. (For those who read the full article, I feel obligated to note an issue with the prior paragraph, which cites three experiments in which participants acting as peer reviewers caught 25%-30% of major flaws to argue that "reviewers mostly didn’t notice" major flaws. In those three cited experiments, researchers added [8](https://jamanetwork.com/journals/jama/article-abstract/187748), [9](https://journals.sagepub.com/doi/full/10.1258/jrsm.2008.080062), and [10](https://www.sciencedirect.com/science/article/abs/pii/S019606449870006X) major errors to the research work participants reviewed. Reviewers need only find one major error to conclude a work should not be published as is, and after documenting two or three of 8-10 errors, participants conducting publication review can surely be expected to stop adding additional evidence for rejection.)

[^attention]: A few senior scientists have told me that "curation" is also an important function of peer review, as attention is a scarce resource much as the costs of printing, mailing, and storing paper once were. This argument ignores that humans have already adopted to the deluge of information available after publishing costs approached zero in the prior decades. We acquired new technologies and social practices. We have adjusted how we distribute information and recommendations. It seems as unlikely that young scientists would opt in to have a council of elders determine what is worth reading.

[^funding]: Similarly, funding agencies, such as the US National Science Foundation (NSF), must ultimately decide which of their reviewed projects to fund. But, they do not glorify exclusivity. The last NSF reviewing instructions I received noted that it was "unfortunate" that most proposals cannot be accepted. In this author's experience, the NSF works far harder than most publication review committees to ensure reviews are assistive to authors.  Their instructions encouraged reviewers to provide detailed comments to improve future proposals, and I've witnessed NSF staff ask panelists to write kinder and more constructive reviews.

<!-- NSF Instructions I received:
  The rating is NOT the most critical aspect of a review -- of far more value are comments on the
  advantages and disadvantages of the proposal.  Unfortunately, the large majority of proposals
  do not get funded.  Detailed comments give the PIs of declined proposals information about
  how to improve future proposals, and help NSF Program Directors justify our funding decisions.
-->