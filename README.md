# Rejecting *Reject* in Scientific Peer Review

Efforts to reform peer review too often critique *how* we choose which works to accept and which works to reject, when we should be examining the wisdom of building peer review around acceptance decisions. Concerns about accept criteria are, indeed, legitimate. Criteria that favor experiments with ‘significant’ outcomes selects for chance results and for irresponsible scientific practices that amplify chance results.[^evaluating-experiments-by-their-outcomes] Reviewers' appreciation for work that is familiar in approach, language, and style creates a bias against new entrants and underrepresented groups.

The subjectivity of our acceptance criteria is evidenced by our inconsistency; in experiments evaluating peer review itself, half of the works accepted by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] Scientists and the public use peer review to gauge the integrity of research, but our inconsistency in segregating accepts makes it a less credible signal. Peer review is also often the best chance for objective experts to catch problems that a work's authors may have overlooked before that work is published, but the well-known inconsistency of outcomes makes it too easy for authors to disregard feedback they find inconvenient. Adopting less subjective criteria to increase consistency could help...to a point.

But, no matter how we improve acceptance criteria, authors fearing rejection will optimize for acceptance criteria, sometimes at the cost of understandability, replicability, or accuracy. Even when reviewing with improved criteria, once we start leaning towards an accept/reject decision we are likely to bias the content of our reviews to include observations that best justifies our decision as fair. Peer review cannot segregate works into accepts and rejects without reducing objectivity. We need to offer forms of peer review that do not segregate.

<!-- 
To get the benefits of peer review without losing Yet, We need peer review to identify problems they might have overlooked and to signal the work's credibility to their audience. We need to allow them to get the ben To empower researchers to write for their intended audience (readers, not reviewers) and enlist reviewers to help them, we should offer forms of peer review that do not segregate. -->

We should embrace *informative* peer review, in which reviewers provide feedback exclusively to inform the work's authors and audience, without attempting to compress the work's value into a boolean accept/reject outcome.

The public, and scientists ourselves, rely on peer review outcomes as a signal of a work's integrity, but informative peer review does not provide accept/reject signals. We use acceptances signals of scientific integrity, but acceptances are not the only, or best, way for reviewers to convey. Informative peer review can signal integrity so long as authors employing it to share the reviews they receive if and when they share or publish their work.[^social-contract] When we encounter deeply-flawed work when performing informative review, we can choose words that indicate to a lack of credence equivalent to, or greater than, that which might be inferred by one learning the work had been rejected by competitive peer review.[^rejects-invisible-if-unpublished]

While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.
<!-- 

Implicit obligation to update review if author has made good faith effort to address feedback
  Move here and delete from below
  The skills we already use in collaborations are skills we can use to persuade authors to improve their work: explaining how revisions could improve the work and even instructions to help realize those improvements. We can also promise to update our reviews to acknowledge genuine improvements.
-->

Informative peer review signal credibility more accurately and convincingly than through accepts and rejects. It can separate the extremely meticulous from the merely acceptable and separate the glaringly wrong from the insufficiently convincing. In contrast, acceptances are to often treated as blanket endorsements, causing readers to overlook important limitations and shortcomings of accepted works. Whereas rejecting a work protects its audience from being misled at most once, peer review that helps its audience understand how research can mislead helps immunize that audience against being misled in the future. Further, when researchers can submit and publish their work with missteps disclosed, rather than censored, they help reveal which types of mistakes are most common so that others can also learn to recognize and avoid them.

<!-- #### We should promote the adoption informative peer review -->

Any scientific researcher *can* and *should* contribute to informative peer review. The skills required are ones we already use when conducting our own research: examining others' work and sharing our insights into it for a broad audience. The skills we already use in collaborations are skills we can use to persuade authors to improve their work: explaining how revisions could improve the work and even instructions to help realize those improvements. We can also promise to update our reviews to acknowledge genuine improvements.

<!-- When we perform informative peer review, we are serving to help a work's audience understand the work better and to help the work's authors improve it. Serving people is more fulfilling than serving the -->
We *should* volunteer to perform informative review because it can make science less biased, more inclusive, more just, and kinder.

<!-- Our selection of observations to share in our reviews will be less subject to [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) to support an accept or reject decision if there is no such decision to make. Supporting informative review can make science more inclusive be are naturally biased to accept work by authors similar to us, whether it's because the work addresses problems similar to our own or because the writing uses a dialect more familiar to us, and thus there is a structural bias to reject work by those underrepresented in science. -->
We can be kinder to authors required to reject the majority of submissions and write feedback to support those outcomes. We can also be kinder to our fellow reviewers when we do not have to haggle over outcomes. 

Offering constructive feedback that isn't packaged with a reject outcome could also reduce the number of aspiring scientists who exit the field because they are uncomfortable promoting their work as important, or who are too “thin-skinned” or “insufficiently perseverant” to dedicate time and effort to positioning their work as worthy of praise. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback biases survival in favor of those best able to ignore evidence, which are exactly those we should least want to be scientists.[^selection-of-scientists]
<!-- We should make science more welcoming of those who see zero sum games as antithetical to an endeavor meant to increase *everyone's* knowledge. -->

Offering informative peer review would also make science more just. Abdicating the power to grant prestige to some works and reject others can help us avoid the corrupting influence of such power, especially when that power can be wielded anonymously.[^anonymity] Instead of reinforcing imbalances by granting those invited review committees power over others' work, informative peer review tasks reviewers with reducing knowledge imbalance between authors and their audience.

But we should not just work to offer informative peer review to others, we should take advantage of it ourselves.

<!--
    In the short-run, informative peer review prior to competitive peer review makes a work more competitive.
    In the long-run, informative peer review allows us to separate the cooperative goals of furthering science from the competitive goals of status and stratification. Separate the industry of science from the act of science.
    (Informative peer review provides public goods: helping authors improve their work and helping others understand it.
    Competitive peer review supports the stratification system that supports the industry of science, but doesn't serve science itself.)
 -->

Submitting for informative review would reduce the time we need to wait for the feedback we need to improve our work, because the bulk of that time we wait when submitting for competitive review is needed to compare competing submissions. Reviewers evaluating those competing submissions must read them all, and discuss them with other reviewers, before casting judgment and sharing feedback. In contrast, a reviewer conducting an informative review need only read one work and may choose to share feedback without evaluating other work or awaiting others' reviews. They can provide feedback while our work is still fresh in our mind, not after we have begun to feel unfamiliar with what we wrote. Opting for informative scientific peer review and self publishing could dramatically accelerate the pace of scientific discourse, allowing time-critical work to be scientifically evaluated in days instead of months.

We could even start earlier, seeking informative review of our experimental designs, and improving those designs, *before* conducting experiments.  The current practice, in which reviewers criticize experimental designs after the experiments have already been conducted, is unnecessarily wasteful, causes unnecessary risk, and is inevitably frustrating for researchers who receive suggestions they can no longer use.

Many of us will still be obligated to seek the acceptance of a *competitive* review, as career advancement often requires us to accumulate acceptances from exclusive publications and present at exclusive conferences[^reviewers-are-biased-sample]. However, we can still benefit from informative review. The feedback we receive from informative review can strengthen our work before submission for competitive review. Sharing the reviews and evidence of our efforts to satisfy those reviewers can strengthen our case for acceptance. If we have satisfied informative reviewers we are less likely to question our own competence when competitive reviewers reject our work due to disinterest.

The only way to truly address the harms of competitive review is for science to re-evaluate our dependence on it and the stratification system we've built on top of it. It is we who collectively perpetuate the systems that demand that we rank peers' research, and in so doing rank our peers themselves. Introducing marginally less subjective forms of peer review only serves to perpetuate this system and the illusion that we might someday find criteria that are objective.

<!-- Counting acceptances may save us all some time in evaluating which papers are worth reading and which candidates are worth interviewing, but the tax this system imposes on our time costs much more time than it saves. -->

But introducing informative review is the catalyst we need to trigger that reform. By demonstrating that scientific peer review need not produce accepts and rejects can we subvert the very pretense that ranking others is service to science. By using informative review to improve our work and establish its scientific credibility prior to competitive review, we can more accurately account for the time and effort we lose chasing acceptances to accumulate and maintain status and prestige. 


Still, those planning to submit for competitive review will not be able to write solely for their intended audience, but will still need to write to court acceptance from reviewers and minimize our risk of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection without even thinking about it, often not realizing how differently we might write were it not necessary to do so. We open our papers with introductions written to convince reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works[^citation-counts] by potential reviewers, use unnecessary jargon coined by potential reviewers, and employ frameworks invented by potential reviewers, all to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them.

And only by bypassing competitive review can we be free of from the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to appeal to a new peer review committee with different members, expectations, interests, and paper formatting rules. Each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch, even if work is effectively unchanged from submission to prior reviewers.




---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: See [recommended readings](./Recommended-Readings.md#the-neurips-2021-consistency-experiment) for a summary of the The NeurIPS 2021 Consistency Experiment or go directly to the [primary source](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/).

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Informative peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.

[^rejects-invisible-if-unpublished]: The originally-intended audience of a work may not not see the reviews if authors choose not to publish it.

[^anonymity]: Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.

[^citation-counts]: Reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).


[^reviewers-are-biased-sample]: Alternatively, conferences could offer presentation spaces of various sizes and allocate those spaces by audience interest instead of competitive peer review. Review committees are, after all, a biased sample of likely attendees.
