# Rejecting *Reject* in Scientific Peer Review

<!-- Concepts to add? 
  off-ramp from zero-sum science
  shadow-discourse

  THE HARMS OF PRETENDING A SYSTEMICALLY-BIASED SYSTEM IS OBJECTIVE,
  (SEE AI, HOUSING, ETC)
-->

Using peer review to segregate research into accepts and rejects undermines its scientific purpose: improving the quality and credibility of research. Scoring research work, which we must do to rank or segregate it, amplifies the biases in our feedback. Embracing forms of peer review that produce no scores can make science more objective, just, and efficient.
<!-- We can improve the  science more objective, inclusive, and efficient by conducting forms of peer review that do not segregate, rank, or score. -->

Our belief in our expertise and in the quality of our own work and expertise biases our view of others' works. We are inclined to favor works that are familiar in approach, language, and style to our own; as well as works that confirm our ‘expert’ opinions. The harms caused by these biases depend on *how* we review works.

When reviewing work to produce a score, our implicit biases can influence our scores, but their influence doesn't stop there, because our scoring intentions influence the written feedback we provide. Even when we strive to be objective in making observations and reporting them, those observations that confirm the fairness our intended scores (and our views of ourselves as fair) are likely to seem more salient. When we have no scores to justify, we are more likely to find salient those observations that might most help the authors improve their work or help non-experts understand it.

Since peer reviewers are typically selected from those who have had historical success in a field, our implicit biases disadvantage those who are new to a field, those who are underrepresented within a field, and those who do work that challenges our strongly-held beliefs and prejudices. They not only get lower scores, but less useful feedback.

Our subjectivity as reviewers is evidenced by experiments evaluating the replicability of peer review outcomes, in which half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] Those who receive enough acceptances to survive in in academic careers often refer to this inconsistency as *randomness*, falsely perpetuating a myth of fairness: unlike randomness, systemic biases are not distributed fairly, do not diminish with repeated trials, and get worse overtime as today's peer reviews determine who survives to become tomorrow's reviewers.

The subjectivity with which acceptances are granted undermines their (mis)use as a signal of credibility.

Legitimizing systems we know to be systemically biased in harmful, and we do this when we ignore the biases and inconsistencies of peer review. We produce CVs that list acceptances and acceptance rates. We warn the press to be cautious about results that have not been accepted for publication, even when journalists solicit more independent expert reviews than a peer review committee would. 

**HERE** Reforming acceptance criteria helps perpetuate the misuse of a systemically-biased process as a signal of whether research is credible ().

<!-- Focusing on acceptance criteria will only aggravate the harms that result when we treated systemically-biased system as if they were objective, as we do when treating acceptances as a signal of the credibility and value of a research work. -->


<!-- Because the our biases are subconscious and independent of review criteria, they cannot be corrected by improving review criteria. Acceptance decisions will always be systemically biased. -->

<!-- Yet, no matter how much we work reduce the subjectivity acceptance criteria, using peer review to segregate works into accepts and rejects is, in and of itself, enough to corrupt our objectivity. -->





<!-- We are systemically biased to favor works by those similar to us and that support our existing beliefs, and against works from outsiders and those with conflicting beliefs. -->


To make science more objective, credible, and just, we must embrace forms of peer review that do not score works, rank works, or segregate works into accepts and rejects.

Specifically, we should conduct *informative* peer review, producing only written feedback to inform a work's authors and their audience. We should write feedback to help help authors improve their work and to identify issues they may have overlooked. We should provide their audience expert insights that complement the authors' by adding clarity, balance, and external perspective. To ensure the audience has access to these insights, *informative* peer review requires authors to share reviews whenever the work is shared.[^social-contract]
<!-- Those insights, not a score or decision, should be the product preserved to stand the test of time. Such feedback is valuable when it helps -->

<!-- We can ensure the audience has access to reviewers' insights into a work's credibility, and their other insights, by requiring authors who submit work for informative review to commit to sharing the resulting reviews whenever they publish their work or share it with others.[^social-contract] -->

When we refuse to compress work's integrity into a binary *accept* or *reject* outcome we can avoid many of the familiar hazards that result: rejections are misconstrued as indictments; acceptances are misconstrued as blanket endorsements, leading audiences to overlook works' shortcomings. Informative peer review gives us the opportunity to differentiate extremely meticulous findings from those that are somewhat credible, those that are the glaringly wrong, and everything in between. We can explain that some of the findings in a work may be significantly more credible than other findings in the same work, rather than having to assign a single outcome to the bundle.[^rejects-invisible-if-unpublished]

<!-- When conducting informative peer review, we do not compress a work's integrity into a binary outcome. We do not produce rejections that might be misconstrued as indictments. We do not produce acceptances that might be misconstrued as blanket endorsements, leading audiences to overlook works' shortcomings. Rather, we produce reviews that can differentiate the extremely meticulous findings from those that are somewhat credible, those that are the glaringly wrong, and everything in between---noting that some results may have very different credibility than others within the same work.[^rejects-invisible-if-unpublished] We can ensure the audience has access to those insights by requiring authors who submit work for informative review to commit to sharing the resulting reviews whenever they publish their work or share it with others.[^social-contract] -->

Informative peer review can also help the audience of the work we review become more savvy. Reading reviewers concerns about the credibility of a result can help them recognize results that would cause similar concerns in the future, whereas when a work is rejected that audience would not have the same opportunity to learn.


<!-- Whereas rejecting a misleading work protects its audience from being misled at most once, informative peer reviews can help their audience understand how other research might mislead them in the future. -->
When authors can seek feedback without risk of rejection, they may be more likely to disclose their missteps, publish those disclosed missteps, and in so doing help others learn which types of mistakes are most common and learn to avoid them.


Make no mistake, this proposal is subversive to entrenched interests. But, it is in the interest of science.


Conducting scientific peer review without accepts and rejects is a subversive act because many of our scientific institutions exploit the pretense that that segregating works into accepts and rejects is service to the advancement of science.

We tally acceptances to rank each other, even though doing so reinforces systemic biases.


<!--

Instead of focusing on action, could transition into "beyond objectivity"

-->

#### Conducting informative review in service to others

<!--  Whereas competitive forms of peer review require us to compare the subjective importance of others' research questions, informative review only asks scientists to use the same skills used to conduct and present their own research.   -->

<!-- Any scientific researcher *can* and *should* perform informative peer review in service to others.  -->

<!--  The skills needed to conduct informative peer review are those we use to put conduct our own research: examining others' work and sharing our insights into it for a broad audience. The skills we already use in collaborations are skills we can use to persuade authors to improve their work: explaining how revisions could improve the work and even instructions to help realize those improvements. (We should also update our reviews when authors make improvements that deserve acknowledgement.) -->

<!-- When we perform informative peer review, we are serving to help a work's audience understand the work better and to help the work's authors improve it. Serving people is more fulfilling than serving the -->
Conducting informative peer review allows us to serve science without the hazards of ranking and rejecting much (often most) of the work we review. We need not fear the corrupting influence of power over others' work once we have abdicated it.[^anonymity] Whereas competitive peer review can reinforce imbalances, giving those who already have status in a research community power to distribute status to work they like, informative peer review tasks us with helping authors and with reducing the knowledge imbalance between authors and their audience.

[^anonymity]: The corrupting influence of power over others' work can be compounded when we exercise that power anonymously.  Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.


 <!-- We can also be kinder to our fellow reviewers when we do not have to haggle over outcomes.  -->

#### Seeking informative review of our own research work

One reason to seek informative review of our own research work is to increase its likelihood it will be accepted when subsequently submitted for competitive peer review. Constructive feedback from disinterested experts identifies problems and opportunities to strengthen our work. By addressing reviewers's concerns, and attaching their revised reviews (as required), we make it harder for subsequent reviewers to argue that our submission was prepared without sufficient expert input or that new concerns, not apparent to prior experts, should have been anticipated and addressed before submitting for competitive review.

We can get feedback more quickly from informative review than competitive review because comparing submissions causes the bulk of the wait: reviewers must read all competing submissions, discuss them with other reviewers, and often make decisions before sharing feedback with authors. In contrast, a reviewer conducting an informative review need only read one work and may choose to share feedback without evaluating other work or awaiting others' reviews. Informative review can return feedback while our work is still fresh in our mind, not after we have begun to feel unfamiliar with what we wrote.

We could even seek informative review *before* conducting experiments, receiving expert feedback on experimental designs, and improving them, before incurring unrecoverable costs and taking risks. The current practice of conducting peer review only after the experiments have already been conducted is unnecessarily wasteful and frustrating for all involved.

#### xxx

Seeking informative review prior to submitting for selective review also serves a greater purpose: subverting the pretense that segregating works into accepts and rejects advances sciences, or is somehow necessary to the pursuit of scientific knowledge. We advance science when we examine a work's integrity, helping authors to find missteps they may have overlooked, and when we provide feedback that helps clarify their work so others can better understand it. Segregating works confers prestige to those whose works are deemed noteworthy or interesting, but it does not further science. We tally acceptances to evaluate scientists, and this may provides a veneer of objectivity when evaluating candidates for jobs or promotions, but aggregating systemically-biased outcomes does not remove subjectivity, it only disguise it.




We chase acceptances to accumulate prestige, and tally others' acceptances to gauge their success, but these pursuits are tangential to scientific progress and often deleterious to it. When we serve on peer review committees that produce accepts and rejects, ours is the outsourced labor that allows institutions to score candidates for jobs or promotions without understanding their work. Tallying acceptances provides a veneer of objectivity, but aggregating systemically-biased outcomes only disguises our subjectivity.


We advance science when we identify problems authors may have overlooked in their work, when we provide constructive feedback that them improve that work, and when we help others understand and evaluate the credibility of that work. Producing accepts and rejects serve a tangential purpose.

Journals and conferences that brand themselves as “highly-selective” do so by increasing the ratio of rejects to accepts. Institutions screening job candidates by their acceptances, effectively outsourcing the job of reading and understanding candidates' contributions to peer review committees. Some purport this makes hiring more objective, but aggregating systemically biased outcomes does not produce objectivity, only deniability .

Informative review isolates the act of evaluating the credibility or research work and improve that work from  it from  ---it does not.  Informative review protects integrity and provides expert feedback. Competitive review allows us to rank each other and supports the stratification system by which our industry evaluates scientists.

Many of us will still be obligated to seek the acceptance of a *competitive* review, as career advancement often requires us to accumulate acceptances from exclusive publications and present at exclusive conferences[^reviewers-are-biased-sample]. However, we can still benefit from informative review.


But introducing informative review is the catalyst we need to trigger that reform. By demonstrating that scientific peer review need not produce accepts and rejects can we subvert the very pretense that ranking others is service to science. 

Still, those planning to submit for competitive review will not be able to write solely for their intended audience, but will still need to write to court acceptance from reviewers and minimize our risk of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection without even thinking about it, often not realizing how differently we might write were it not necessary to do so. We open our papers with introductions written to convince reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works[^citation-counts] by potential reviewers, use unnecessary jargon coined by potential reviewers, and employ frameworks invented by potential reviewers, all to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them.

And only by bypassing competitive review can we be free of from the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to appeal to a new peer review committee with different members, expectations, interests, and paper formatting rules. Each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch, even if work is effectively unchanged from submission to prior reviewers.


We constructive feedback not attached to a reject outcome could help reduce the number of aspiring scientists who exit the field because they are uncomfortable promoting their work as important, or who are too “thin-skinned” or “insufficiently perseverant” to dedicate time and effort to position their work as worthy of praise. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback biases survival in favor of those best able to ignore evidence, which are exactly those we should least want to be scientists.[^selection-of-scientists]



<!-- Opting for informative scientific peer review and self publishing could dramatically accelerate the pace of scientific discourse, allowing time-critical work to be scientifically evaluated in days instead of months. -->


  <!-- When we submit for competitive review, we will be obligated to attach the reviews .  The feedback we receive from informative review can strengthen our work before submission for competitive review. Sharing the reviews and evidence of our efforts to satisfy those reviewers can strengthen our case for acceptance. If we have satisfied informative reviewers we are less likely to question our own competence when competitive reviewers reject our work due to disinterest. -->



<!-- We should make science more welcoming of those who see zero sum games as antithetical to an endeavor meant to increase *everyone's* knowledge. -->

<!--

    In the short-run, informative peer review prior to competitive peer review makes a work more competitive.

    In the long-run, informative peer review allows us to separate the cooperative goals of furthering science from the competitive goals of status and stratification. Separate the industry of science from the act of science.
    (Informative peer review provides public goods: helping authors improve their work and helping others understand it.
    Competitive peer review supports the stratification system that supports the industry of science, but doesn't serve science itself.)
 -->
But we should not just work to offer informative peer review to others, we should take advantage of it ourselves.

In the short term, informative peer review gives us an avenue to seek constructive feedback to validate our work, improve it, and even give it a leg up before it must compete for acceptances.

In the long term, it helps us reckon with the stratification system the scientific community has built on top of competitive peer review.





<!-- The feedback we receive from informative review can strengthen our work before submission for competitive review. Sharing the reviews and evidence of our efforts to satisfy those reviewers can strengthen our case for acceptance. If we have satisfied informative reviewers we are less likely to question our own competence when competitive reviewers reject our work due to disinterest. -->

The only way to truly address the harms of competitive review is for science to re-evaluate our dependence on it and the stratification system we've built on top of it. It is we who collectively perpetuate the systems that demand that we rank peers' research, and in so doing rank our peers themselves. Introducing marginally less subjective forms of peer review only serves to perpetuate this system and the illusion that we might someday find criteria that are objective.

<!-- Counting acceptances may save us all some time in evaluating which papers are worth reading and which candidates are worth interviewing, but the tax this system imposes on our time costs much more time than it saves. -->


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: See [recommended readings](./Recommended-Readings.md#the-neurips-2021-consistency-experiment) for a summary of the The NeurIPS 2021 Consistency Experiment or go directly to the [primary source](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/). The first experiment was for [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS), which had a call for papers with a section for evaluation criteria which stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity.” The 2021 call for papers listed no evaluation criteria.

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Informative peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.

[^rejects-invisible-if-unpublished]: The originally-intended audience of a work may not not see the reviews if authors choose not to publish it.

[^citation-counts]: Reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).


[^reviewers-are-biased-sample]: Alternatively, conferences could offer presentation spaces of various sizes and allocate those spaces by audience interest instead of competitive peer review. Review committees are, after all, a biased sample of likely attendees.
