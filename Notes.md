# Notes

<span id="to-rank-each-other"></span>
#### How we use exclusionary peer review to rank fellow scientists

Scientists' ability to hurl research over the gates of exclusionary peer review determines our career trajectory in science's stratification system. We signal our success on our curriculum vitaes by listing each paper with the brand of the journal or conference that accepted it.

<span id="replicability"></span>
#### On the reproducibility of peer review

Past attempts to test whether peer include the [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/), and its 2014 predecessor, which duplicated the review process for a fraction of papers. Per the 2021 report:

> In 2014, 49.5% of the papers accepted by the first committee were rejected by the second (with a fairly wide confidence interval as the experiment included only 116 papers).  This year, this number was 50.6%.

The experimental methodology is noteworthy for removing independent variables that might cause differences between the replica review processes. The replication took place in parallel (removing potential temporal differences), as part of the same conference peer review process (removing potential differences in reviewer pool, culture, and criteria). Again, in their words:

> During the assignment phase of the review process, we chose 10% of papers uniformly at random—we’ll refer to these as the “duplicated papers.”  We assigned two Area Chairs (ACs) and twice the normal number of reviewers to these papers.  With the help and guidance of the team at OpenReview, we then created a copy of each of these papers and split the ACs and reviewers at random between the two copies.  We made sure that the two ACs were assigned to two different Senior Area Chairs (SACs) so that no SAC handled both copies of the same paper.  Any newly invited reviewer for one copy was automatically added as a conflict for the other copy.  We’ll refer to the SAC, AC, and reviewers assigned to the same copy as the copy’s “committee.”  The papers’ committees were not told about the experiment and were not aware the paper had been duplicated.

This methodology assured that papers were reviewed to a single peer review committee's reviewing standards and with reviewers from the same pool of candidates (a reviewer might be reviewing a paper from replica A and a different paper for replica B).

We can expect even lower reliability under less controlled circumstances, such as when authors re-submit their work to a different peer review committee. Re-submissions means work will be evaluated by a committee with traditions and expectations, different pools of reviewers, with new prior work released in the intervening time, and with changes in what research is fashionable over the passage of time.

<span id="publishing-reviews"></span>
#### Publishing reviews

The [social contract](./Notes.md#social-contract) of *elucidative* peer review requires authors who submit their work for review to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, illuminating what may have changed since each reviewer last updated their review.

This social contract may be informal, or peer review committees may formalize it as a condition of review.

Peer review committees or others may provide a platform on which the work and/or its reviews can be updated, or this process may be manual.


<span id="faster"></span>
#### Elucidative peer review is faster


When authors opt for elucidative peer review, we in their audience get access to their research sooner—we we do not need to wait for authors to submit and re-submit their work until finding a peer review committee that consider it worthy of publication.

<span id="research-audience"></span>
#### Elucidative peer review allows authors to write for their audience

TO DO

<span id="review-audience"></span>


<span id="social-contract"></span>
#### Regarding “*the social contract of elucidative peer review*’"
Peer review committees may choose to formalize this social contract by requiring authors to commit to linking their published paper to the reviews.
<!-- Is this contractual upon requesting reviews? --> 





<span id="speculation"></span>
#### Regarding “*researchers are pressured by reviewers to go beyond factual reporting*”
As an example of how exclusionary peer review pressures authors to speculate beyond their factual findings, I will quote two of the three reviewers from my recent co-authored submission to the *2022 Symposium on Usable Privacy and Security*.

Reviewer B complained that data tables with the exact wording survey questions and answers be replaced with charts to make space for the “*critical reflection of the results and a discussion of the larger implications of the results.*”

Reviewer C wrote “*I'd like to read the authors' thoughts about the findings and how they inform future research and product development.*”

<span id="procedural-television-dramas"></span>
#### Regarding “*procedural television dramas*”

The justice system's clear separation of duties between investigation and advocacy (prosecution) has been drilled into the public's awareness for over two decades by the ubiquitous procedural television drama [Law and Order](
https://en.wikipedia.org/wiki/Law_%26_Order) and its spin-offs. Each episode starts with following introductory narration:
>In the criminal justice system, the people are represented by two separate yet equally important groups: the police who investigate crime, and the district attorneys who prosecute the offenders. These are their stories.
