# What We Each Can Do to Fix Peer Review
<!-- # Rejecting *Reject* in Scientific Peer Review -->

<!-- Concepts lost
   Fashion show
-->

The most scientifically valuable forms of peer review do not require us to score others' research work, rank it, or decide whether it is worthy of publication. Yet, the most broadly available form of peer review is publication review, which makes scoring central and, in so doing, can introduces bias. We can make each make science more objective, equitable, and efficient by making ourselves at least as accessible to conduct *scoreless* peer review as we are for publication review.

*Scoreless* peer review enhances objectivity because scoring others' work activates and amplifies our implicit biases: we are predisposed to appreciate works that are familiar in approach, language, and style to our own work; we are predisposed to trust results that confirm our existing beliefs and hopes. Scoring requires us to quantify our sense of how much the work should be appreciated, trusted, or otherwise valued, which we cannot answer in isolation of our biases. These biases can then propagate from our scores to our written feedback. Even when our written feedback is strictly factual, we can only report a small fraction of the facts we observe, and facts will seem more salient when they confirm the fairness of our scoring intentions. 
<!-- Newcomers, underrepresented groups, and other outsiders may not only get lower scores, but worse feedback. -->

We do not know the extent to which our biases influence our peer review scores, decisions, and feedback, but the scant evidence available certainly does not suggest that we are objective. In experiments by the NeuroIPS conference in 2014 and again in 2021, half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] It's fashionable to refer to this inconsistency as ‘*randomness*’, but doing so is unscientific and harmful, perpetuating the assumption that our inconsistency is unbiased (randomness is uniformly distributed) and diminishes in impact over time and scale. The word ‘randomness’ dismisses the possibility of systemic biases against underrepresented researchers and unpopular ideas, which grow worse as today's peer reviews determine who survives to become tomorrow's peer reviewers.

Scoreless peer review cannot remove our biases, but it does remove a trigger that may activate and amplify those biases. By excising scores, rankings, and acceptance decisions, scoreless peer review can excise our need to make our feedback written consistent with them.

And while scoring may be a necessary evil for selecting works for exclusive publications, scoreless peer review can serve the two *scientific* purposes most often used to motivate peer review: improving research work and evaluating the credibility of research work.

Improving research work is one of the reasons many of us already use and conduct a form of scoreless peer review with students and colleagues. We scrutinize their work to catch errors, offer clarifications, and suggest other ways to make the work better (often prior to publication review). We ask colleagues to do the same for us. Such *author-assistive* review makes our work better without scoring.

Alas, social norms of offering author-assistive review to those we know, and publication review to everyone else, make author-assistive peer review by experts far less broadly accessible than publication review by experts. Insiders have access to experts willing to donate their time while others may have no choice but to get feedback by submitting their work to journals or conferences. Those insiders get constructive feedback quickly, whereas outsider may wait months longer only to receive feedback written to justify a rejection outcome.[^mostly-rejections]

If we want to ensure our well-intentioned service to others does not unwittingly disadvantage underrepresented groups and other outsiders, we should each make ourselves at least as accessible for *author-assistive* peer review as we are for publication review.[^pre-experimental-review]

[^pre-experimental-review]: We should also offer to assist authors *before* they conduct experiments, helping them refine their hypotheses and improve their experimental designs prior to incurring the risks, costs, and time the experiments require. The current practice of reviewing and rejecting experimental designs at publication time, after the costs of conducting the experiment have been sunk, is wasteful and frustrating for all involved. Pre-experimental review and registration of experimental designs can increase the chance that null results will be published. A [2018 examination](https://psyarxiv.com/3czyt) of 127 bio-medical and psychological science registered reports (pre-registered studies) showed that a 61% null result rate, as opposed to a typical 5-20% null result rate for studies published in venues that did not require pre-registration.

We can also use scoreless peer review to address the need to evaluate the credibility of research work. Using publication review for this purpose is problematic. Publications' acceptance criteria include factors such as ‘novelty’ and ‘significance’ that are not only unrelated to credibility, but subvert it by rewarding unscientific practices and chance results.[^evaluating-experiments-by-their-outcomes] Acceptances are too often misconstrued as endorsements that lead readers to overlook key limitations. Outright fraud is rarely detected by publication review and often benefits from its endorsement. And, since publication review requires scoring, under-represented groups and other outsiders will find it harder to establish that their work is credible through publication than insiders will.

The scoreless alternative is *audience-assistive* peer review, in which we address our feedback to the work's audience to help them understand the work and evaluate its credibility. Writing gives us the opportunity to explain why some results are more credible than others. Writing gives us the nuance to differentiate extremely meticulous findings from those that are somewhat credible, those that are the glaringly wrong, and everything in between.[^rejects-invisible-if-unpublished]. Writing gives us the opportunity to help the audience learn to become better at identifying the strengths and weaknesses of research results on their own. Audience-assistive reviews become part of science's open discourse and so there is more opportunity to recognize and mitigate any bias in the review process than when work is scrutinized as part of a shadow discourse. When authors write for audience-assistive review, they can write for their audience, rather than to appeal to reviewers, improving the quality of science's public discourse.[^written-for-reviewers] Authors can get feedback more quickly through audience-assistive review because it does not require reviewers to coordinate on decisions, though we may choose to update our reviews after learning from others' reviews. 

We should each make ourselves as accessible for *audience-assistive* peer review as publication review, as doing so will give all scientists a faster and more objective means to have the credibility of their work vetted, and in so doing will reduce the disadvantage that under-represented groups and outsiders have in establishing the credibility of their work when publication review is the only way to do so. We should make audience-assistive review available to authors immediately after their research is public, adding our feedback to the public discourse, and before authors are willing to share it publicly, which we may condition on our feedback being made public when the research is.[^anonymity]

We should also volunteer for the administrative work required to make both forms of scoreless peer reviewing available to the public, as we do when serving as editors or program chairs for journals and conferences. 

Dedicating time to scoreless peer review may force us to be more selective in volunteering for publication review. As more work is submitted to conferences with audience-assistive peer reviews already weighing in on their credibility, we may re-evaluate whether our time is well spent helping conferences choose which works should be presented, or if conference attendees might be better served if the conferences allocated presentation space by asking *them* which works they were interested in seeing. As more works our submitted to journals after audience-assistive peer review, we may re-evaluate whether this service is the best way to draw attention to important works that might otherwise go unnoticed.[^fashion-shows]

[^fashion-shows]: As it becomes harder to differentiate value of selecting research for publication and selection clothing for fashion show runways, more of us may find that publication review feels incompatible with our role as scientists seeking objective truths.


<!-- We may find it easier to reckon with concerns about whether publication review is a fair way to evaluate people. -->

Many of us will still feel obligated to perform publication review for exclusive journals and conferences and to submit our work to them. Many our our employers, and prospective employers. make hiring and promotion decisions by examining which journals and conferences are work is published at and which we serve as reviewers. Reforming peer review has been a challenging collective action problem for the scientific community because even those of us senior enough to be able to eschew chasing publication status collaborate with those whose careers depend on it.

The key to reforming peer review is to disentangle its competing goals of improving a work, establishing the work's credibility, and evaluating whether the work is important enough to warrant inclusion in publications that draw attention to the work and advance careers of its authors. Broadening access to author- and audience-assistive review empowers scientists to invoke the scientific functions of peer review in isolation from the problematic non-scientific functions. While offering scoreless review is an important reform on its own, we may spur further change by undermining the pretense that scoring research is essential to, or even compatible with, the scientific goals of peer review.

<!-- By serving the scientific functions of peer review outside of publication review, we can chip away at the scientific veneer that justifies the use of our time to publication review to distribute attention and career advancement. -->


---

Stuart Schechter ([@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist)) wrote this with suggestions and feedback from  [Joseph Bonneau](https://jbonneau.com/), [Sauvik Das](https://www.hcii.cmu.edu/people/sauvik-das). [Mary L. Gray](https://marylgray.org/), [Cormac Herley](https://cormac.herley.org/), [Jon Howell](https://research.vmware.com/researchers/jon-howell), and [Michael D. Smith](https://seas.harvard.edu/person/michael-smith).

[^anonymity]: The corrupting influence of power to score others' work can be compounded when we exercise that power anonymously. Audience-assistive peer review can performed anonymously, but there may be less reason to when reviews do not produce scores or outcomes.

[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/) followed the earlier experiment at [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS). The 2014 call for papers stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity” whereas the 2021 call for papers listed no evaluation criteria.

[^mostly-rejections]: Since journals and conferences compete to be exclusive, those submitting to the publications with the most experts can expect the highest rejection rates.

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Audience-assistive peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of publication peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.

[^rejects-invisible-if-unpublished]: Further, when rejected work goes unpublished they disappear from scientific discourse, we can't learn from any mistakes that were made, and so the same mistakes and failed experiments can be re-run over and over. We bias research in favor of chance results.

[^written-for-reviewers]: Research works today are often more written for reviewers than a work's true audience, at a cost both to that audience and to authors. For example, reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the number of citations to grow out of hands. Bibliographies have evolved like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).



[^figure-out-where-to-put-this-footnote]: Whereas publication review can reinforce knowledge asymmetries, audience-assistive feedback is designed to reduce knowledge asymmetries, reducing the knowledge gap between authors and audience.