# Peer Review Should Elucidate, not Exclude.

We scientists say we practice peer review to protect the scientific integrity of research: employing objective experts to identify its flaws, evaluate its credibility, and ensure that it does not confuse or mislead. Since we have convinced the press and public that peer review is a signal that research is credible, we *should* make paramount the goal of preventing research from misleading.

But in much of science, what we are actually doing when we peer review research is to rank it relative to other research competing for attention and, by extension, to rank our fellow scientists relative to each other. Peer review committees are run by journals and conferences, each of which has its own brand, and which signal brand prestige by being selective in accepting submissions. They use reviewers as gatekeepers to determine which research will be of interest to their audience and which should be excluded. Being selective often requires rejecting some submissions of sufficient integrity that lack such subjective qualities such as ‘*novelty*’, ‘*impact*’, or that particularly nebulous catch-all (really a *drop-all*) of having a ’*significant contribution*’. Unsurprisingly, the decisions made by exclusionary peer review processes are themselves not replicable: when the same set of papers is reviewed by two different sets of reviewers in the same community, there can be substantial disagreement on which papers should be accepted and rejected [*](./Notes.md#regarding-the-reproducibility-of-peer-review). As scientists, our ability to hurl our research over the gates of *exclusionary* peer review determines our career trajectory in science's stratification system. We signal our success on our curriculum vitaes by listing each paper with the brand of the journal or conference that accepted it.

Perversely, we scientists continue to practice exclusionary peer review even though the function of limiting what can be published is now vestigial to science. There were once financial forces that made information exchange expensive and constrained how much could be published: when research was printed on paper, traveled by mail, and took up space in libraries. While we may have once needed reviewers to determine what was worth publishing, we do not in an era where publication is effectively free. As the information available to us has grown exponentially, and we have faced a deluge of information competing for our attention, we acquired new tools to filter it to focus on what interests us, such as search engines and social media sharing. We no longer need peer reviewers to be arbiters of fashion, curating which research is eye-catching enough to model on science's most exclusive runways and which to leave hanging backstage in science's sweaty dressing rooms.
<!-- Perhaps for the slide version of this talk "Runways are an odd slightly-raised surface for scientists to choose to die on." or "When I ask fellow scientists whether they really want to be arbiters of fashion, I'm often surprised how many will choose a runway as the hill they're willing to die on." -->

There are myriad ways in which exclusionary peer review undermines scientific writing and scientific integrity, especially when research is excluded for being subjectively uninteresting. So when we say we conduct peer review to prevent scientists from misleading ourselves and the public, we our misrepresenting our actual practice of peer review in a way that misleads ourselves and the public. We should aspire to peer review for the purposes we say we do, not as we actually do.

In this article, I will
 1. detail why using *exclusionary* peer review to vet scientific research makes writing less scientific, is antithetical to scientific integrity, and is harmful to those who practice science;
 2. examine what we can learn from the integrity protections in other social systems for evaluating hypotheses and establishing shared beliefs; and
 3. argue that purely *elucidative* peer review can better serve scientists and scientific integrity.

## Exclusionary peer review harms science and scientists

There are many ways using peer review to rank research, and determine which research is interesting enough to publish, harms scientific writing and causes science to mislead ourselves and others. (See the [recommended readings](./Recommended-Readings.md) for additional background on these flawed scientific practices.)

**Excluding research biases the body of published results**. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences of *publication bias* the potential for false discovery, especially when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective (at least at p<0.05)—and get published.

**Favoring interesting research causes authors to write less scientifically**. The need to make research interesting inspires authors to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. It provokes authors to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. It tempts authors to deceive others, and even themselves, into believing that hypothesis tests they conducted on data they found interesting were planned before they had seen the data (or [HARKing](./Recommended-Readings.md#HARKing-Hypothesizing-After-the-Results-are-Known)).

It motivates authors to aggrandize their research to look more important. In some fields (including mine: see [example](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.

**Making reviewers gatekeepers changes the audience research is written for**. To craft papers to be accepted by reviewers, authors must make sacrifices in serving their intended audience. For example, as authors we often add volumes of citations of no value to anyone but our reviewers. We do so to signal to reviewers that we are experts in the field and to reduce the chance of being faulted for failing to cite a reviewer's favorite work. Since reviewers expect papers to have at least as many citations as previously-accepted work, we add even more unnecessary citations when writing on a relatively new topic for which there is little related work. As authors write to conform, not inform, papers have grown to meet reviewers' ever increasing expectations for citation counts, and with cited work receiving less explanation of why its relevant to help the non-expert reader. We also conform by maximizing the amount of information that can fit of a page limit (to maximize the "contribution" needed for acceptance), and using our full page limit, rather than publishing shorter (or longer) papers.

**Exclusionary peer review delays the spread of knowledge**. Because comparing competing submissions takes time, authors must often wait to submit their research until a common deadline, reviewers need time to read a cohort of submissions to compare them against each other, and committees need time to collectively discuss which papers to accept. These delays can add months to each submission cycle. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish. In many fields, the value of research can decline significantly while it awaits a single round of exclusionary peer review (extreme cases including the detection and prevention of emerging diseases or of systems distributing election misinformation).

**Delays disrupt authors' work and degrade our memories**. Time decays our ability to recall how we conducted our experiments, our observations during the experiments, and the tooling we used to process data and document our results. By the time have access to reviewers' feedback identifying what we failed to document, we may have already forgotten details that reviewers noticed we had elided. Coming back up to speed on that which we do remember also takes time and effort. Unless we are lucky enough to have our work accepted on its first submission, we must disrupt our other work to revise our submission to target a different publication, with different reviewers, with different interests, paper formatting rules, and other expectations.

**Stratification built on exclusionary peer review undermines scientific standards**. Every journal and conference has different standards of acceptance. Many journals and conferences publish work they know to be flawed to meet financial or attendance goals. The lowest strata publications may provide the pretense of peer review while accepting any paper for which authors pay publication fees. Even the most prestigious conferences in my field are known to override some reviewer's integrity concerns if others believe the work will draw interest and attention that might otherwise go to competing conferences. Yet, many outsiders are unfamiliar with these different standards, or are unaware that scientific standards vary so broadly depending on the publication.

<!-- --- -->

**Exclusionary peer reviewing causes broader harms to science**.

Conferences and journals that reject a majority of papers will necessarily provide more negative feedback to the community than positive feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. 

The predominance of negative feedback biases who becomes a scientist. The scientific community loses talented aspiring scientists who are uncomfortable promoting their work as important, or who are too "thin-skinned" or "insufficiently perseverant" to discard feedback that conveys their work is uninteresting and unimportant. Those more comfortable promoting the importance of their work or whose sheer self confidence allows them to dismiss negative feedback survive. Some have even argued that natural selection favors scientists whose "poor" methods "produce the greatest number of publishable results" which leads to "increasingly high false discovery rates" [[Smaldino and McElreath]](./Recommended-Readings.md/#the-natural-selection-of-bad-science).  Moreover, this baseline expectation of negative feedback can also hamstring efforts to improve diverse, equity, and inclusivity efforts in science: since existing peer-review gatekeepers may not share or value the epistemic background and expertise of aspiring scientists from underrepresented backgrounds, aspiring scientists who produce work more familiar to the gatekeepers are more likely to escape their ire.

Having countless exclusionary peer review committees of different strata also burdens us with unnecessary work as reviewers. We must re-review papers that had already met scientific-integrity requirements when previously rejected. Our efforts to provide constructive feedback are discarded by authors who assume that what their work really needs is a new set of reviewers with different subjective expectations.

Being part of a system that produces toxic feedback for authors is also toxic to us as reviewers. We review alongside those who abuse the subjectivity and anonymity of peer review. We witness the trauma suffered by recipients of toxic feedback while we are obligated to protect those creating the toxins.

Without alternatives to exclusionary peer review, we each must choose between shirking our scientific duty to review others' work or becoming ever more complicit in the system that exposes authors to unnecessarily toxic feedback.

## We can learn from other systems for establishing shared beliefs

Peer review is part of a larger system of rituals that scientists use to build consensus around beliefs. In reevaluating peer review, it is worth stepping back to evaluate how other systems establish consensus beliefs.

Justice systems investigate and seek to test hypotheses where lives hang in the balance: did someone ("a suspect") commit a criminal offense for which they should be punished. As anyone familiar with [procedural television dramas](./Notes.md#regarding-procedural-television-dramas) knows, the criminal justice system separates the roles of investigating crimes from those who prosecute the offenders, advocating for the conclusion of guilt. Since those who choose to investigate a suspect, and put in effort to conduct the investigation, may become invested in believing the hypothesis of guilt, someone more objective evaluates the collected evidence to decide whether it is strong enough to support advocating for the hypothesis of guilt (prosecution). Then, the prosecutor must advocate to a panel of members of the public, a jury, who make the final judgement.

From the perspective of those in criminal justice – with all its unjust flaws that corrupt and bias it – how much more corrupt science must seem; first, for allowing researchers to both investigate a hypothesis and advocate a conclusion; and second, for placing final judgement in the hands of anonymous investigators, some of whom may have advocated for (or still be advocating for) similar or competing conclusions. Science is not served when we peer review encourages scientists to draw judgments beyond that which is immediately inferrable from the evidence presented. Nor is it served when peers' insights into research are condensed into a boolean indicator of credibility that we present as if it represented some broader scientific consensus.

We can also learn from journalists, who are also investigators who face decisions about what facts to treat as shared truths and who must reckon with their larger role of establishing shared beliefs. Those journalists who strive for impartiality (a valuable, if endangered, species) try to let the observations they report speak for themselves.

For a model of how we might prevent research from misleading without gatekeeping what gets published, we might look to how the most diligent journalists report on new scientific research: they reach out to multiple objective scientific experts, ask those experts to speak to the credibility of research and the conclusions that might be drawn from it, and quote those experts so that readers know exactly what they said. When experts use language that may be unfamiliar to a broader audience, the best journalists help make it understandable to that audience.

The model of investigative journalism is *elucidative*: it informs the audience about observed disagreements rather than declaring the winner.

## Peer review need only investigate and elucidate

Our practice of submitting research for exclusionary peer review, and gatekeeping for exclusionary peer review, demands that we scientists be investigators, prosecutors (advocating for a conclusion), and jurors. The latter two roles should give us pause. Science is more objective and credible when our role as investigators comes first.

The better we are able to investigate and elucidate our findings, the better others can draw accurate conclusions and build a shared consensus around them.

When conducting research as scientific investigators, we should be wary of any obligation to advocate for conclusions, especially those that promote the significance or importance of our results. When acting as peer reviewers to investigate others' work for errors, misleading statements, or other shortcomings, we should also be wary of any obligation to go beyond elucidating our findings, such as drawing conclusions about whether the work is important or significant.

To best serve science, we should thus conduct purely *elucidative* peer review, rather than drawing conclusions about whether work should be published. In elucidative peer review, we should address our concerns about how the research might mislead or confuse to the audience of the research—those who we want to protect from being mislead or confused. Our reviews should be published in tandem with the research to accompany it. In other words, our reviews should be *accessible* to the audience of the research both technologically and linguistically—the audience should be permitted to retrieve the reviews and should be able to understand them.

In addition to elucidating for our audience (as journalists are limited to), we can attempt to persuade authors to revise their work. The audience is best served if authors can to eliminate our causes for concern or, when that's not possible, disclose those concerns on their own. When persuasion works and authors successfully address our concerns, we can revise our audience-directed feedback to remove our former concerns or note that we are satisfied with how authors addressed them.

As authors, we may disagree with those who review our work, as we do today. We may choose to rebut their concerns within the work or through separate commentary. As reviewers, we can assist authors by noting if we find others' reviews inaccurate or prejudicial. We may also weigh in even when not formally reviewing, either through our reviewing systems or through other means of public discourse. Elucidating disagreements amongst scientists gives our audience a more accurate and nuanced view of how science works than summarizing it into a single ‘*accept*’ or ‘*reject*’ decision.

When we are ready to publish work we have authored as scientifically peer reviewed in the elucidative model, we are required to share the reviews with audience. If we want to publish a revision before the reviews are updated in response, or if reviewers are unwilling or unable to respond, we must also share the last version(s) reviewed by each reviewer, illuminating what we have changed since they last weighed in.

Elucidative peer review doesn't reach final conclusions because science takes a long time to build consensus around our confidence in a result, and whether a hypothesis can ever be truly lain to rest. The pretense of finality we attach to exclusionary peer review can only serve to erode the public trust in science.

## Elucidative peer review is better for scientists and those we serve

Adopting elucidative peer review would benefit us whether we are authoring, reviewing, or consuming research.

**As reviewers**, participating in elucidative peer review allows us to perform a service to authors, and to serve science more broadly, without being complicit in ranking and exclusion. Elucidative reviewing gives our reviews a higher purpose and broader audience than exclusionary reviewing. It also lightens our reviewing workloads, as authors need not resubmit their work repeatedly in search of reviewers who consider their work interesting enough to publish—each work needs only one set of reviewers.

**As authors**, preparing work for elucidative peer review allows us to write for our intended audience, prioritizing scientific content over reviewers' subjective interests. We can expect feedback to be more constructive and timely. We need only wait for reviewers to read and respond to our work to get feedback, and we need only wait until we are satisfied that we have sufficiently addressed reviewers concerns to publish it.

Using elucidative peer review, we could even submit our experimental methodology for feedback, using reviewers' insights to improve those experimental designs *prior* to running experiments and reporting results. We could then seek further feedback after completing the research. Currently, peer review only reveals experimental design flaws after we have spent all the resources needed to run an experiment, and often after key co-authors have graduated or started other jobs.

**As consumers** of research, elucidative peer review provides objective insight into the credibility of that research, written to be accessible to non-experts. When authors opt for elucidative peer review, we in their audience get access to their research sooner—we we do not need to wait for authors to submit and re-submit their work until a peer review committee declares it interesting.

We should also demand more control over the decisions that guide our access to research that interests us, rather than what others think should interest us. For example, we could eschew conferences where an elite group of peer reviewers decide what which presentations we want to see, and instead opt for those that ask us which prospective presentations we would want to see and allocate speaker slots and lecture hall sizes to give the audience what we're actually interested in.

**Those who stand to lose** by the adoption of elucidative peer review are not scientists or the public we serve, but the journals and conferences which use the veneer of scientific integrity to perpetuate exclusionary peer review in service of their brands. They are the churches and temples whose survival has depended on our perverse willingness to perpetuate an anachronistic bloodletting ritual—to collectively torture ourselves with our own negative feedback. 

They stand to lose their near monopoly on the distribution of scientific credibility that makes us seek their approval, and their near monopoly on reviewing service.

We may feel some regret hastening the demise of the journals whose acceptances marked important steps up in our rankings in science's stratification system, and whose peer review committees provided us with camaraderie. We should recognize that it is common for the victims in abusive relationships to become dependent on the approval of our abusers, and to even want to protect them.

But exclusionary peer review and the institutions that perpetuate it do not serve science. To preserve the credibility of the scientific endeavor, we must reject the notion that they do.

---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). Those who might dismiss Stuart's arguments by concluding that he is sort of mad scientist would be wise to consider an alternate hypothesis, that they may have only seen him mildly aggrieved. You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).

---

[Acknowledgements](./Acknowledgements.md) | [FAQ](./FAQ.md) | [Notes](./Notes.md) | [Recommended readings](Recommended-Readings.md)
<!-- em — , en –   …  -->