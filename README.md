# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower us with others' expertise and insights to make us all better scientists and communicators.

But our current peer review practices often hinder science instead of advancing it.

One such practice is *competitive* peer review, in which we pit research works against each other to vie for inclusion in exclusive publications, or for other forms of recognition. We pick winners using criteria that inevitably undermine scientific integrity[^undermine-integrity] while maintaining the pretense of protecting integrity. We overestimate our competence in evaluating these criteria; in the few well-designed experiments of competitive peer review, [half of the works accepted by one review were rejected by a parallel review](./Recommended-Readings.md#the-neurips-2021-consistency-experiment).

We need an alternative to *competitive* peer review. Yet, even if we conducted peer review work non-competitively and for integrity alone, we could not objectively or fairly segregate works to *accepts* and *reject*s. Scientific integrity has many dimensions and compressing them to a boolean is inherently subjective[^subjective-integrity], necessarily lossy, and thus likely to misled. We need to revisit the very premise that *scientific* peer review should produce *accepts* and *rejects*. This requires us to overcome the misleading intuition that rejecting flawed work is the most effective means to protect scientific integrity.

Rejecting work can prevent flawed or misleading work from reaching its audience—that audience cannot be misled by that which they cannot see. However, the prophylactic power of *reject* is limited to a single submission. The rejected work's authors may still re-submit their work unchanged elsewhere or publish it without peer review. Rejecting work cannot ensure its flaws will be fixed.

Further, our power to reject has harmful side effects, including that our choice not to use that power is an implicit endorsement of the works we accept. That implicit endorsement may make a work's audience overlook important limitations and shortcomings. When we do successfully use the power to reject work to censor works with significant missteps, we conceal those missteps from those who might study them or learn from them. We make it harder for science to learn from our collective mistakes and immunize ourselves against them.

Building peer review around the decision to accept or reject work can also subvert our objectivity as reviewers, as we must advocate and justify our outcomes. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that decision. We may unfairly ignore observations that support the competing outcome, as such observations create dissonance with our faith in our objectivity and competence in making fair choices.

To address the harmful side effects of the power to reject the work we peer review, we should promote forms of peer review that do not produce *accept* and *reject* outcomes.

Specifically, we should offer peers the option of *informative* review, which produces only written evaluations of a work to inform the work's authors *and* audience.

We can protect scientific integrity through informative peer review by obligating those who submit their work for review to share those reviews if and when they publish their work (or otherwise share it with others).[^social-contract]  Within those reviews, we can expose any flaws we encounter in their work, especially if those flaws undermine a works' integrity. We can help the work's audience understand how the work might mislead them, instead of trying to censor the work so that they never see it.

When we encounter deeply-flawed work, we can choose words that signal a lack of credence equivalent to, or greater than, that which might be inferred by one learning the work had been rejected by competitive peer reviewers.[^rejects-invisible-if-unpublished] When evaluating particularly meticulous work, we could explain how the work exceeded the minimum standard one might infer from an *accept*. Whereas peer review that concludes with a *reject* only protects a work's audience from being misled once, peer review that explains how research can mislead could help to immunize that audience against being misled in the future.

By conducting a form of scientific peer review that requires us to abdicate the power to reject the work we review, we can avoid being corrupted by that power, avoid decisions that may send misleading signals, and avoid censoring our collective mistakes from those who would learn from them.

Some authors will inevitably still receive written feedback they disagree with, dislike, or even feel misleads their audience. While authors would remain obligated to share feedback they disagreed with they could note and address the disagreement, encourage other reviewers to address the disagreement, or even invite outside experts to do so.

Anyone who produces scientific work can, and should, contribute to informative peer review. The skills required, evaluating research and explaining our findings to inform a broad audience, are skills we already need to conduct and present our own research, and so we should already be comfortable using them. We can also use our existing skills as scientific collaborators to help persuade authors to improve their work: explaining the purpose and value of proposed improvements, providing constructive feedback to make improvements easier to realize, and offering the promise to update our reviews to acknowledge genuine improvements.

By offering informative peer review to others we can help make science kinder, and make ourselves feel better about our role as reviewers.  When peer review is exclusively informative, reviewers can write for the benefit of the work's potential audience, not to convince fellow reviewers to agree on an acceptance decision, nor to convince authors that their decision is fair. We could expect to collectively produce more constructive and positive feedback, at least in aggregate, because we would not need to concoct reasons to justify rejecting the majority of papers. Informative peer review would provide a source of constructive feedback on competently-executed work that many might find unfashionable or uninteresting—such as replication studies.<!-- Whereas competitive peer review introduces a power differential between the authors of a work and those assigned to review it, informative review is designed to reduce the knowledge differential between the authors of the work and their audience.  -->

By offering informative peer review we could also help make science more inclusive. We are naturally biased to accept work by authors similar to us, whether its because the work addresses problems similar to our own or because the writing uses a dialect more familiar to us. We are thus naturally biased to reject work by those underrepresented in science. We could reduce the rate at which aspiring scientists leave the profession because they are uncomfortable promoting their work as important, or who are too “thin-skinned” or ”“insufficiently perseverant” to discard feedback that argues their work is unimportant. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback biases survival in favor those best able to ignore evidence.[^selection-of-scientists] We would also make science more welcoming of those who see zero sum games as antithetical to an endeavor meant to increase *everyone's* knowledge.

Offering informative peer review would also make science more just. Whether reviewers remain anonymous or not, abdicating the power to reject work would help prevent the abuses that result from that power being wielded anonymously.[^anonymity] Instead of giving reviewers powers over authors, informative peer review tasks reviewers with reducing knowledge imbalance between authors and their audience. <!-- Informative peer review of giving reviewers power over authors, it employs reviewers to reduce the imbalance between authors and less knowledgeable members of their audience. -->

We should also make use of informative peer review to seek feedback for our own work when the option is available to us, prior to competitive review if not in place of it.

Preparing work for informative review would empower us to write for our intended audience, not to court acceptance from reviewers and minimize our risk of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection without even thinking about it, often not realizing how differently we might write were it not necessary to do so. We open our papers with introductions written to convince reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works by potential reviewers, use unnecessary jargon coined by potential reviewers, and employ frameworks invented by potential reviewers, all to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them. Reviewers' exploding expectations for citation counts, and our need to exceed those expectations when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

Submitting work for *informative* review would also reduce the time we need to wait for feedback, because the bulk of that time needed to compare competing submissions. Reviewers evaluating competing submissions must read them all, and discuss them with other reviewers, before casting judgement and providing feedback. A reviewer conducting an *informative* review need only read one work and may choose to share feedback without awaiting others' reviews. They can provide feedback while the work is still fresh in our mind, mot after we ourselves are unfamiliar with it. Opting for *informative* peer review could dramatically accelerate the pace of scientific discourse, allowing time-critical work to receive feedback in days instead of months.

*Informative* peer review could allow us to get feedback on our experimental designs before conducting our experiments.  We could fix those designs before incurring the costs and risks of running the experiments. The current practice, in which reviewers criticize experimental designs after the experiments have already been conducted, is unnecessarily wasteful and frustrating.

We could also save time and effort if informative review made it unnecessary to submit some of our work for competitive review. Bypassing competitive review would eliminate the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to appeal to a new peer review committee with different members, expectations, interests, and paper formatting rules. Each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch, even if work is effectively unchanged from submission to prior reviewers.

We could more easily skip competitive review if we had opportunities to present our work that didn't require it. We could encourage conferences to provide speaker slots to research works based on attendee's interest, employing rooms of different sizes to accommodate works with niche audiences, works with large audiences, and those in between. Allowing attendees' choices to be dictated by competitive reviewers biases those choices in favor of those that appeal to a disproportionately senior and powerful subset of a community[^reviewers-are-biased-sample].

Many of us who offer and submit for informative peer review will face pressure to subsequently submit that work for competitive review, either for our own career score cards or for those of our collaborators. We should still take advantage of informative review. We can use informative feedback to help strengthen our work before submission for competitive review, and the reviews themselves can strengthen our case for acceptance by subsequent competitive review. The satisfaction of having satisfied one set of reviewers can also help insulate us from the negative feedback inherent to competitive peer review, and help us differentiate subsequent feedback that indicates a lack of interest from that which questions our competency.

Over time, we can hope that building community around informative peer review can help science better reckon with the trade-offs and harms of competitive review. It would allow us to recognize that science's reliance on competitive peer review is a choice we make out of our need to rank research and to rank each other, not one that is needed to serve or protect science itself.

---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^undermine-integrity]: Rejecting work that isn't ‘*significant*’ encourages authors to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: *Informative* peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of *informative* peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.

[^rejects-invisible-if-unpublished]: The originally-intended audience of a work may not not see the reviews if authors choose not to publish it.

[^anonymity]: Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates” [[Smaldino and McElreath]](./Recommended-Readings.md/#the-natural-selection-of-bad-science).


[^reviewers-are-biased-sample]: Reviewers are disproportionately older and more powerful than the general population of conference attendees. We should be suspect of those who believe that they are best able to choose which work should be modeled on science's more exclusive runways and which to leave hanging backstage. Runways may be engineered hills, but they are still poor hills for scientists to choose to die on.
