# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower authors with others' expertise and insights to make us better scientists and communicators.

But the rituals we call ‘peer review’ often *hinder* science, not advance it.

We pit peers' research work against each other to compete for inclusion in exclusive publications or other forms of recognition. Competitive peer review maintains the pretense of serving science and protecting scientific integrity, but the criteria used to pick winners inevitably [undermines integrity](./The-Harms-of-Exclusionary-Peer-Reivew.md). Further, we cannot pick winners objectively or reliably; in the few well-designed experiments of competitive peer review, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). 

The problem is not merely that we review work competitively or evaluate works for more than scientific integrity. We could not objectively categorize works into *accepts* and *rejects* even if we could accept every work that met some threshold of scientific integrity. Scientific integrity has many dimensions, and compressing it to a scalar, and then to a boolean, is lossy and perilous. Segregating works into *accepts* and *rejects* also not necessary.

The appeal of having the option to *reject* work is that preventing deeply-flawed work from being published appears the surest way to prevent it from misleading its intended audience. However, a *reject* cannot prevent authors from self-publishing that work, or seeking out a publication with less rigorous standards of peer review. When we do succeed in censoring work in which authors have taken avoidable missteps, we conceal those missteps from those who might study or learn from them, making it harder for science to learn from our collective mistakes.

Further, the power to reject has consequences beyond any one work that we might feel compelled to censor. The existence of *reject* necessitates the alternative of *accept*. Accepting work that we could have rejected will inevitably be construed as an endorsement, yet accepted works are rarely without limitations and shortcomings that many readers will overlook.

The power to reject work can also corrupt our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that outcome. We may be unfairly ignore observations that support the alternative, as they create dissonance with our belief that we chose the fair objective outcome.

The surest way to avoid being corrupted by the power to reject work is to abdicate that power. The surest way to avoid the problems of compressing our evaluation of a work's integrity into a boolean is not to compress it. The surest way to ensure our mistakes are not forgotten when we censor them is not to censor them. The surest way to avoid unfairly picking winners and losers is not treat scientific advancement as a competition.

The product of *scientific* peer review need not, and should not, be a decision to *accept* or *reject* a work.

Peer review should instead produce a written evaluation of the work be shared with the work's authors *and* audience. Peer review should inform a work's potential audience, not decide whether the work is shared with that audience.

When predicating peer review with the requirement that authors share the reviews when publishing it, peer reviewers could address their evaluations directly to the work's potential audience, to ensure that audience is not disadvantaged by lacking reviewers' expertise, or by lacking the time to read as diligently as reviewers are expected to. For deeply-flawed work, reviewers could make their concerns apparent to the work's audience, signalling a lack of credence similar to that which would be conveyed by a *reject* (though that audience may not see the reviews if authors choose not to publish). When reviewing particularly meticulous work, reviewers may elucidate why its credibility exceeds that which would be signalled by a mere *accept*. Conducting investigations and elucidating findings for a broad audience are essential skills for the conduct of science, and so they make a sensible foundation for peer review that uses scientists as its workforce.

In addition to addressing the work's audience, reviews could also act to persuade authors to improve the work. Reviewers could advocate for the value of improvements, provide constructive feedback to reduce the effort needed to make those improvements, and promise to update their reviews to acknowledge those improvements.

All forms of peer review can produce feedback that is negative and that authors may find unfair. While authors would be required to share it, they could rebut it, encourage other reviewers to rebut it, and invite the broader scientific community respond.

---

This *illuminative* form of scientific peer review should be strictly isolated from competitive peer review, and other forms review that segregates works into *accepts* and *rejects*.

Isolating *illuminative* peer review would ensure that reviewers address their evaluation to the work's potential audience, not to convince fellow reviewers to agree that their conclusion of *accept* or *reject* is more correct, or to convince authors that such an evaluation is fair.

Employing exclusively *illuminative* review would allow authors to write their papers to serve their intended audience, not to court acceptance from reviewers. When writing papers for competitive review, we cannot help but try to appease reviewers' expectations. We write to justify its importance to reviewers who were assigned to read it. We may use jargon to signal that we are knowledgeable. We may also signal expertise by providing many times more references than our audience actually needs, and cite works by likely reviewers to curry favor with them. We could better serve our intended audience if writing were not an exercise in avoiding rejection.

Employing exclusively *illuminative* review would reduce the time authors need to wait for feedback, because most of the current delays is needed to compare competing submissions. In competitive peer review, reviewers must review a set of submissions, discuss each submission with other reviewers, and collectively determine outcomes for all submissions before authors receive feedback. Reviewers providing *illuminative* reviews could provide feedback immediately after evaluating a work without biasing themselves by reading others' reviews. We could peer review works of time-critical work in days instead of months.

Authors who employ exclusively *illuminative* peer review would save time and effort for both themselves and reviewers. Opting out of competitive review eliminates the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to a new peer review committee with different members, expectations, interests, and paper formatting rule. Even if work is unchanged, each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch.

Embracing *illuminative* peer review could also make science kinder and more inclusive. Not all feedback will be positive, but it would surely be less negative if the majority of reviews did not need to be written to justify the choice to reject. Providing peer review that is not competitive could make science more inclusive, as reviewers are naturally biased to accept work by authors similar to themselves, whether its because the work addresses problems similar to their own or whether its because the writing is in a dialect similar to their own. Illuminative peer review does not conflate reviewers' subjective disinterest in a work, or sense that it lacks significance, with a lack of scientific integrity.

---

Illuminative scientific peer review and competitive peer review can coexist.



---

We could expect more constructive and positive feedback from *elucidative* peer review, at least in aggregate, because reviewers would not need to [concoct reasons to reject the majority of papers](./The-Harms-of-Exclusionary-Peer-Reivew.md/#negative-feedback).

