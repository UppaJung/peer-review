# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower authors with others' expertise and insights to make us better scientists and communicators.

But the rituals we call “peer review” often *hinder* science, not advance it.

We “peer review” research for inclusion in exclusive publications or other forms of recognition. Inclusion is often competitive. We pit research works against other research works under the pretense of serving science and protecting scientific integrity, but the criteria for winning inevitably [undermine integrity](./The-Harms-of-Exclusionary-Peer-Reivew.md). Further, our ability to reliably pick winners and losers is suspect; in the few well-designed experiments of peer review, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). 

The problem is not merely the subjectivity of comparing works against each other, or measuring which significant enough to be worthy of recognition. We could not objectively categorize works into *accepts* and *rejects* even if we did not pit research works against each other and if scientific integrity were our only review criteria. There are many dimensions to scientific integrity, and compressing them into a boolean is lossy and perilous. It's also not necessary.

It might seem like the power to reject work is the most sure way to protect scientific integrity. Preventing deeply-flawed work from being published seems the surest way to prevent it from misleading its intended audience. However, a *reject* cannot prevent authors from distributing results, possibly with the endorsement of a subsequent, less-diligent, review process.

Further, the power to reject has consequences beyond a single work that we might want to censor. The existence of *reject* necessitates the alternative of *accept*, and accepting work that we could have rejected will inevitably be construed as an endorsement. Yet, accepted works are rarely without limitations and shortcomings that many readers will overlook. Another consequence of our power to *reject* work, and prevents its publication, is that rejections conceal authors' missteps from those who might study or learn from them, making it harder for science to learn from our collective mistakes.

Our objectivity as reviewers can also be corrupted by our power to *accept* or *reject* a work. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that outcome. We may be unfairly ignore observations that support the alternative, as they create dissonance with our belief that we chose the fair objective outcome.

The surest way to avoid being corrupted by the power over whether a work is accepted or rejected is to abdicate it. The surest way to avoid the problems of compressing our evaluation of a work's integrity into a boolean is not to compress it. The surest way to ensure our mistakes are not forgotten is to record them. The surest way to avoid unfairly picking winners and losers is not treat scientific advancement as a competition.

The product of *scientific* peer review should not be a decision to *accept* or *reject* a work, but a written evaluation of the work be shared with the work's authors *and* audience. Peer review should inform, not decide.

When requiring that authors share the reviews when publishing it, peer reviewers can address their evaluations directly to the work's potential audience, to ensure that audience is not disadvantaged by lacking reviewers' expertise, or by lacking the time to read as diligently as reviewers are expected to. For deeply-flawed work, reviewers could make their concerns apparent to the work's audience, signalling a lack of credence similar to that which would be conveyed by a *reject* (though that audience may not see the reviews if authors choose not to publish). When reviewing particularly meticulous work, reviewers may elucidate why its credibility exceeds that signalled by a mere *accept*. Conducting investigations and elucidating findings for a broad audience are essential skills for the conduct of science, and so they make a sensible foundation for peer review as well.

In addition to addressing the work's audience, reviews can also act to persuade authors to improve the work. Reviewers can advocate for the value of improvements, provide constructive feedback to reduce the effort needed to make those improvements, and promise to update their reviews to acknowledge those improvements.

This *illuminative* approach to scientific peer review should be strictly isolated from peer review that segregates works worthy of publication, or other recognition, from works to *reject*. It should be isolated from *competitive* review.

Isolating *illuminative* peer review ensures that reviewers address their evaluation to the work's potential audience, not to convince fellow reviewers that their conclusion of *accept* or *reject* is more correct.

Isolating *illuminative* review is also essential to ensure that authors write their papers for their audience, not reviewers. And, whether we realize it or not, when writing papers for competitive review, we often write to give reviewers what they expect, not to give our audience what they need. We often start introductions by explaining why the subject of our work is important, lest reviewers who would not otherwise read it question whether our work would have an audience. We may use jargon to signal expertise. We may provide many times more references than we need so as to signal reviewers that we are experts and ensure that reviewers' don't fault us for leaving their work out.

Isolating *illuminative* review reduces the time authors need to wait for reviews because most of the time they currently wait is a consequence of evaluating competing submissions. Reviewers must review a set of submissions discuss these submissions with other reviewers, and make decisions about the full set of submissions before authors receive feedback. When reviewers can read a work in isolation of other works, and do not need to discuss the work with others, they can provide their review immediately after evaluating the work. We could peer review works of critical importance in days instead of months.

Opting for *illuminative* peer review would save time and effort for both authors and reviewers, as it eliminates the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles are a necessary feature of competitive review, but they require diligent authors to revise work for re-submission to a new peer review committee with different members, expectations, interests, and paper formatting rule. Even if work is unchanged, each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch. Authors should still incorporate feedback nad improve their work, but reviewers who have already read the work and provided that feedback can evaluate those changes more efficiently than new ones.



*Illuminative* scientific peer review would make scientific discourse **faster**, more **efficient**, more **inclusive**, **kinder**.




More credible: no more perverse incentives.
More understandable: correct audience.
Faster: no waiting to compare
Efficient: no more reject and resubmit (submit before experiment)
Inclusive: scientific feedback without rejection for outsider status
Kinder: rejection only for status


We could expect more constructive and positive feedback from *elucidative* peer review, at least in aggregate, because reviewers would not need to [concoct reasons to reject the majority of papers](./The-Harms-of-Exclusionary-Peer-Reivew.md/#negative-feedback).

We would still get negative feedback, some of which we may disagree with. We would be obligated to share feedback with disagree with along with our work. We could, however, rebut feedback we disagree with and publish, allowing the broader scientific community to evaluate the merits of each position.
