# What You Can Do to Fix Peer Review
<!-- # Rejecting *Reject* in Scientific Peer Review -->

<!-- Concepts lost

   Authors incentives
   Writing for audience


-->

The most valuable forms of scientific peer review do not require us to score others' work, rank it, or decide whether it is worthy of publication. We can make science more efficient, objective, and equitable by making such scoreless peer review available to more than just our students, colleagues, and friends.

<!-- We can make science more efficient, objective, and equitable by making ourselves available to peer review others' works without evaluating whether those works are publishable, without ranking them, and without doing anything else that would require us to score them. -->

Removing scoring transforms peer review because scoring triggers and amplifies our implicit biases. We are predisposed to appreciate works that are familiar in approach, language, and style to our own work. We are predisposed to trust results that confirm our existing beliefs and hopes. Once triggered, these biases can propagate from our scores to our written feedback, no matter how objective we try to be. Even if provide feedback containing only factual observations, we can only report a small fraction of the facts we observe, and facts that confirm the fairness of our scoring intentions will seem more salient. Newcomers, underrepresented groups, and other outsiders may not only get lower scores, but worse feedback.

It's unclear just how biased we are producing peer review scores and decisions, but the scant evidence available certainly does not suggest we are objective. In experiments by the NeuroIPS conference in 2014 and again in 2021, half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] It's fashionable to refer to this inconsistency as *randomness*, but doing so perpetuates the assumption that inconsistency is unbiased (impacting everyone equally) and diminishes over time. It dismisses the possibility of systemic bias against underrepresented researchers and unpopular ideas, which grows worse as today's peer reviews determine who survives to become tomorrow's peer reviewers.

Removing the act of scoring eliminates a trigger that may activate or amplify our biases. Removing the scores, ranks, and acceptance choices eliminates the need to make our feedback match our (potentially-biased) choices.

Most of us already conduct scoreless peer review when scrutinizing work authored by students and colleagues at their request. We help them catch errors, we offer clarifications, and we suggest other ways to make the work better. <!-- Many of solicit, and provide, such *author-assistive* peer review on short deadlines, sometimes as little as hours, as we prepare work for submission to journals and conferences. --> One reason authors seek to have their work peer reviewed is to help improve it. Those who have experts they can ask favors of get high-quality feedback quickly. Those who do not may have no choice but to get feedback by submitting their work to journals or conferences. They may wait months longer to get feedback that is less constructive often written to justify a rejection outcome. Thus, our social norms of offering *author-assistive* review to those we know, and publication review to everyone else, perpetuates an advantage of insiders over underrepresented groups and other outsiders.

If we want to ensure our well-intentioned service to others does not unwittingly disadvantage outsiders, we should each make ourselves at least as available for *author-assistive* peer review as we are for publication review.[^pre-experimental-review]

[^pre-experimental-review]: We should also offer to assist authors *before* they conduct experiments, helping them refine their hypotheses and improve their experimental designs prior to incurring the risks, costs, and time the experiments require. The current practice of reviewing and rejecting experimental designs at publication time, after the costs of conducting the experiment have been sunk, is wasteful and frustrating for all involved.

We should also provide a scoreless form of peer review through which authors can seek out public scrutiny of their work's credibility. Using publication review for this purpose is problematic. Publications' acceptance criteria include factors such as ‘novelty’ and ‘significance’ that are not only unrelated to credibility, but subvert it by rewarding unscientific practices and chance results[^evaluating-experiments-by-their-outcomes]. Acceptances are too often misconstrued as endorsements that lead readers to overlook key limitations. Outright fraud is rarely detected by publication review, and often benefits from it. And, since publication review can be biased against outsiders, under-represented groups and other outsiders will find it harder to establish that their work is credible than insiders.

An alternative is to offer *audience-assistive* scoreless peer review, in which we address our feedback to the work's audience to help them understand the work and evaluate its credibility. We can provide nuance to help a work's audience learn why some results are more credible than others. We can help the audience become more expert consumers of research. Our reviews can differentiate extremely meticulous findings from those that are somewhat credible, those that are the glaringly wrong, and everything in between.[^rejects-invisible-if-unpublished]

Authors seeking to establish the credibility of their work can request audience-assistive review immediately after making their research public or beforehand, releasing their reviews along with their research. Audience-assistive review does not require coordination between reviewers: they need not rank papers or coordinate on a decision, though they may end up re-evaluating their feedback after seeing others' insights. No form of peer review is sure to be free from bias, but audience-assistive reviews become part of science's open discourse and so there is more opportunity to recognize and mitigate bias than when it is part of our shadow discourse.

If we want to reduce the disadvantage that under-represented groups and outsiders have in establishing the credibility of their work, we should each make ourselves at least as available for *audience-assistive* peer review as we are for publication review.

As we make ourselves more available for *author-* and *audience-assisted* review scoreless, we may still feel obligated to perform publication review for exclusive journals and conferences and to submit our work to them. Many employers use exclusive publications, and service to them, to decide whom to hire and promote, even though publication review reinforces bad scientific practices and amplifies inequities. Publication reform had been a challenging collective action problem because even those of us senior enough to no longer need publications collaborate with those who are obligated to publish. Publication reform has been a collective action problem.

Performing audience-assisted review can help us reform science by undermining the premise that scientific research only becomes credible when it is accepted for publication. It undermines the premise that scoring research serves a scientific purpose, as opposed to serving to help rank and stratify research.

Dividing our time between author-assistive, audience-assistive, and publication review may also help us to be more selective in deciding which publication review committees deserve our time. It may lead us to question whether conferences really need our expertise to decide which work is of interest to attendees, or if we would all be better served if they asked prospective attendees which works they were interested in seeing, using that data to assign appropriately-sized presentation spaces. It may lead us to ask if the prestige of journals is still an efficient way to draw attention to great research by authors whose work might otherwise go unnoticed. And so, through our individual acts, we may make broader reform possible.


<!-- [^anonymity]: The corrupting influence of power over others' work can be compounded when we exercise that power anonymously.  Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper. -->



<!-- 
Journals and conferences that brand themselves as “highly-selective” do so by increasing the ratio of rejects to accepts. Institutions screening job candidates by their acceptances, effectively outsourcing the job of reading and understanding candidates' contributions to peer review committees. Some purport this makes hiring more objective, but aggregating systemically biased outcomes does not produce objectivity, only deniability .
-->

<!-- Counting acceptances may save us all some time in evaluating which papers are worth reading and which candidates are worth interviewing, but the tax this system imposes on our time costs much more time than it saves. -->


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: See [recommended readings](./Recommended-Readings.md#the-neurips-2021-consistency-experiment) for a summary of the The NeurIPS 2021 Consistency Experiment or go directly to the [primary source](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/). The first experiment was for [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS), which had a call for papers with a section for evaluation criteria which stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity.” The 2021 call for papers listed no evaluation criteria.

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Informative peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.

[^rejects-invisible-if-unpublished]: Further, when rejected work goes unpublished they disappear from scientific discourse, we can't learn from any mistakes that were made, and so the same mistakes and failed experiments can be re-run over and over. We bias research in favor of chance results.

[^citation-counts]: Reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).


[^reviewers-are-biased-sample]: Alternatively, conferences could offer presentation spaces of various sizes and allocate those spaces by audience interest instead of competitive peer review. Review committees are, after all, a biased sample of likely attendees.


[^figure-out-where-to-put-this-footnote]: Whereas publication review can reinforce knowledge asymmetries, audience-assistive feedback is designed to reduce knowledge asymmetries, reducing the knowledge gap between authors and audience.