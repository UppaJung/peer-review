# Rejecting *Reject* in Scientific Peer Review

Efforts to reform peer review too often focus on *how* we choose which works to accept and which works to reject. How we choose is a tempting target because we so often rely on criteria with glaring biases and shortcomings. We evaluate experiments by their outcomes, selecting for chance results and for irresponsible scientific practices that amplify chance results.[^evaluating-experiments-by-their-outcomes] We favor work that is familiar in approach, language, and style, disadvantaging new entrants and underrepresented groups. Our lack of objectivity is evidenced by our inconsistency; in experiments evaluating peer review itself, half of the works accepted by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency]

Instead of chasing the myth of criteria that could objectively decide which work to accept and what to reject, we should instead ask *why* scientific peer review should produce accepts and rejects at all. Regardless of criteria, structuring peer review around an accept/reject decision causes those submitting work to optimize to avoid a reject outcome and those reviewing work to produce feedback that best justifies their chosen outcome.

We should embrace peer review that functions only to *inform*, producing written evaluations of a work for the benefit of the work's authors *and* audience. By removing the accept/reject decision, *informative* review would fundamentally change scientific discourse for the better: empowering researchers to write for their intended audience and freeing those reviewing work to make observations unbiased by the requirement to advocate for an outcome or justify that outcome's fairness.

Consumers of science use peer review as a signal of scientific integrity and expect peer review to help prevent research from misleading its audience. To this end, authors who submit their work for informative review could be required commit to sharing those reviews if and when they share or publish their work.[^social-contract] Reviewers could then alert a work's audience when reviewers fear the work might mislead them, as opposed to rejecting work that exceeds some threshold of concern. The informative approach is no less powerful. When we are informatively reviewing and encounter deeply-flawed work, we can choose words that signal a lack of credence equivalent to, or greater than, that which might be inferred by one learning the work had been rejected by other forms of peer review.[^rejects-invisible-if-unpublished] While the requirement to share reviews obligates authors to share feedback even if they believe it's misleading or outright malicious, they could rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.

Informative peer review can improve the credibility of the scientific endeavor in ways that peer review that segregates works into accepts and rejects cannot. Whereas peer review that concludes with a reject only protects a work's audience from being misled once, peer review that explains how research can mislead can help to immunize that audience against being misled in the future. Informative review can also help its audience recognize when works are particularly meticulous. Because informative review does not compress integrity into boolean, it can avoid the resultant hazards, such as when acceptances are seen as implicit endorsements, causing readers to overlook important limitations and shortcomings of accepted works. When researchers can publish works with missteps disclosed, rather than censored, they help reveal the frequency of scientific errors so that others can also learn to recognize and avoid the most common ones.

<!-- #### We should promote the adoption informative peer review -->

Any scientific researcher can, and should, contribute to informative peer review. The skills required, evaluating research and explaining our findings to inform a broad audience, are skills we already need to conduct and present our own research, and so we should already be comfortable using them. We can also use our existing skills as scientific collaborators to help persuade authors to improve their work: explaining the purpose and value of proposed improvements, providing constructive feedback to make improvements easier to realize, and offering the promise to update our reviews to acknowledge genuine improvements.

<!-- When we perform informative peer review, we are serving to help a work's audience understand the work better and to help the work's authors improve it. Serving people is more fulfilling than serving the -->
By performing peer review intended to inform, helping authors and their audiences, we can help to make science kinder and more collaborative than if we only perform peer review in service to the exclusivity of publications. We can expect ourselves and our fellow reviewers to produce more constructive feedback, at least in aggregate, when we do not need to concoct reasons to reject the majority of submissions. The experience of reviewing would also have less conflict, as there would be no need to argue with other reviewers about the right judgement call on acceptance decisions. 
<!-- We can even feel helpful producing reviews of competently-executed work that we might find unfashionable or uninteresting—such as replication studies. -->

Conducting informative peer review would also help us to guard against our own biases. Our selection of observations to share in our reviews will be less subject to [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) if there is no accept or reject decision that they might support or contradict.<!-- counter? -->

By offering informative peer review we could also help make science more inclusive. We are naturally biased to accept work by authors similar to us, whether it's because the work addresses problems similar to our own or because the writing uses a dialect more familiar to us. We are thus naturally biased to reject work by those underrepresented in science. We could reduce the rate at which aspiring scientists leave the profession because they are uncomfortable promoting their work as important, or who are too “thin-skinned” or “insufficiently perseverant” to discard feedback that argues their work is unimportant. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback biases survival in favor of those best able to ignore evidence.[^selection-of-scientists] We should make science more welcoming of those who see zero sum games as antithetical to an endeavor meant to increase *everyone's* knowledge.

Offering informative peer review would also make science more just. Abdicating the power to grant prestige to some works and reject others can help us avoid the corrupting influence of such power, especially when that power can be wielded anonymously.[^anonymity] Instead of reinforcing imbalances by granting those invited review committees power of others' work, informative peer review tasks reviewers with reducing knowledge imbalance between authors and their audience.

But we should not just offer informative peer review to others, we should take advantage of it ourselves.

Getting our first feedback via informative review would reduce the time we need to wait for it, because the bulk of that time we currently wait is used to compare competing submissions. Reviewers evaluating those competing submissions must read them all, and discuss them with other reviewers, before casting judgment and providing feedback. In contrast, a reviewer conducting an informative review need only read one work and may choose to share feedback without awaiting others' reviews. They can provide feedback while our work is still fresh in our mind, not after we have begun to feel unfamiliar with what we wrote. Opting for informative peer review could dramatically accelerate the pace of scientific discourse, allowing time-critical work to receive feedback in days instead of months.

We could even start earlier, seeking informative review of our experimental designs, and improving those design, *before* conducting experiments.  The current practice, in which reviewers criticize experimental designs after the experiments have already been conducted, is unnecessarily wasteful, causes unnecessary risk, and is inevitably frustrating for researchers.

After employing informative peer review, it may still be necessary to submit work for traditional *competitive* review yielding an accept/reject outcome. The feedback we receive from informative review can strengthen our work before submission for competitive review. Sharing the reviews and evidence of our efforts to satisfy those reviewers can strengthen our case for acceptance. If we have satisfied informative reviewers we are less likely to question our own competence when competitive reviewers reject our work due to disinterest.

Still, to get the the most benefit out of informative review, we would do so without plans to subsequently submit for a competitive review. We could then write solely for our intended audience, not to court acceptance from reviewers and minimize our risk of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection without even thinking about it, often not realizing how differently we might write were it not necessary to do so. We open our papers with introductions written to convince reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works[^citation-counts] by potential reviewers, use unnecessary jargon coined by potential reviewers, and employ frameworks invented by potential reviewers, all to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them.

Bypassing competitive review entirely would free us from the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to appeal to a new peer review committee with different members, expectations, interests, and paper formatting rules. Each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch, even if work is effectively unchanged from submission to prior reviewers.

In the short-term, few of us will have the luxury of bypassing competitive peer review entirely. Career advancement often demands that we accumulate acceptances from exclusive publications and present at conferences that use competitive peer review to allocate speaker slots[^reviewers-are-biased-sample]. Science is highly stratified and the stratification systems we use to rank each other are built on competitive peer review. The only way to truly address the harms of competitive review is to reform those systems. 

But introducing informative review is the catalyst we need to trigger that reform. It is we who collectively perpetuate the systems that demand that we rank peers' research, and in so doing rank our peers themselves. Introducing marginally less subjective forms only serves to perpetuate this system and the illusion that we might someday find criteria that are objective. Only by submitting work for informative review prior to competitive review can we account for the time and effort we lose chasing and maintaining status and prestige. Only by demonstrating that scientific peer review need not produce accepts and rejects can we subvert the very pretense that ranking others is service to science.


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: See [recommended readings](./Recommended-Readings.md#the-neurips-2021-consistency-experiment) for a summary of the The NeurIPS 2021 Consistency Experiment or go directly to the [primary source](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/).

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Informative peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.

[^rejects-invisible-if-unpublished]: The originally-intended audience of a work may not not see the reviews if authors choose not to publish it.

[^anonymity]: Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.

[^citation-counts]: Reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).


[^reviewers-are-biased-sample]: Alternatively, conferences could offer presentation spaces of various sizes and allocate those spaces by audience interest instead of competitive peer review. Review committees are, after all, a biased sample of likely attendees.
