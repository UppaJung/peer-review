# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower authors with others' expertise and insights to make us all better scientists and communicators.

But we often practice forms of peer review that can hinder science instead of advance it.

We pit research works against each other to compete for inclusion in exclusive publications or other forms of recognition. We often conduct such *competitive* peer review with the pretense of protecting scientific integrity, but pick winners using criteria that inevitably [undermine integrity](./Notes.md#undermining-integrity). We maintain the pretense that our criteria are objective, but in the few well-designed experiments of competitive peer review, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). x[^replicability]

Yet, even if we reviewed work non-competitively and for integrity alone, we could segregate works into *accept* and *reject* outcomes objectively. Scientific integrity has many dimensions. Compressing them to a boolean is inherently subjective, necessarily lossy, and thus itself likely to mislead and confuse. Rejecting works with flaws is also not the only way, nor even the best way, to protect scientific integrity.

The allure of the *reject* is its prophylactic power to prevent deeply-flawed work from reaching an audience. That audience cannot be mislead by flawed work they never see. However, a *reject* does not cure a work's flaws, and its protection is limited to a single review. The rejected work's authors may still re-submit their work elsewhere or publish it without peer review. When a *reject* does succeed in censoring flawed work, it conceals authors' missteps from those who might study or learn from them, making it harder for science to learn from our collective mistakes. The side effect of our power to reject is that the choose not to use it, the works we accept receive our implicit endorsement, increasing the likelihood that their audience will overlook the limitations and shortcomings that weren't sufficient to justify rejection.

The power to reject work can also corrupt our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that decision. We may unfairly ignore observations that support the alternative, as such observations create dissonance with our belief that our decision is the fair objective outcome.

The surest way to avoid being corrupted by the power to reject work is to abdicate that power. The surest way to avoid the problems of compressing our evaluation of a work's integrity into a boolean is not to compress it. The surest way to ensure censoring our mistakes doesn't lead others to repeat them is not to censor the mistakes at all. The surest way to avoid picking winners unfairly is not to pick them at all. Scientific advancement need not be a competition. The product of *scientific* peer review need not, and should not, be a decision to *accept* or *reject* a work.

Peer review should instead produce evaluations written to *inform* the work's authors *and* audience. To ensure the audience can benefit from those evaluations, authors who submit for this *informative* form of peer review should be [required](./Notes.md#publishing-reviews) to share the reviews if and when they publish their work or otherwise share it with others (similar to [open](./Notes.md#open-peer-review) peer review).

In such *informative* peer reviewer, reviewers could address their evaluations directly to the work's potential audience, to ensure that audience is not disadvantaged by lacking reviewers' expertise, or by lacking the time to read as diligently as reviewers are expected to. For deeply-flawed work, reviewers could make their concerns apparent to the work's audience, signalling a lack of credence equivalent to, or even exceeding, that which would be conveyed by a *reject* (though that audience may not see the reviews if authors choose not to publish). When reviewing particularly meticulous work, reviewers may illuminate why certain dimensions of its credibility exceed that which would be signalled by a mere *accept*. When peer review that is exclusively *informative*, reviewers can write for the benefit of the work's potential audience, not to convince fellow reviewers to agree on an acceptance decision, nor to convince authors that a decision is fair.

*Informative* reviews could still provide feedback authors may not want to hear. While authors would be required to share that feedback, they could rebut that which they disagree with, encourage other reviewers to address the disagreement, or even invite outside experts to do so.

This skills specific to *informative* peer review, elucidating insights to inform a broad audience, are those we should expect fellow scientists to possess and be comfortable using. Our skills as scientific collaborators can also help us persuade authors to improve their work: explaining the value of doing so, providing constructive feedback to make improvements easier to realize, offering the promise to update our reviews to acknowledge genuine improvements. 

---

Increasing the objectivity informative value of scientific peer review is just one of the many benefits of isolating it from segregating *accepts* and *rejects*.

Submitting work for *informative* review would empower authors to write for their intended audience, not to court acceptance from reviewers and minimize their chance of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection out of habit. We open our papers with introductions written to convince disinterested reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works by potential reviewers, and use unnecessary jargon and frameworks introduced by potential reviewers, to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them. Reviewers' exploding expectations for citation counts, and our need to exceed those expectations, have caused the bibliographies we produce to evolve like peacock's trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

<!-- Authors may elide mundane details of experimental designs that reviewers might find tedious, even if those details would be needed to replicate their experiments. Authors may highlight experiments or tests that yielded a statistically significant result and dedicate less space to those that reviewers will find less interesting. Authors may be tempted to deceive others, and even themselves, into believing that hypothesis tests they conducted on data they found interesting were planned before they had seen the data (or [HARKing](./Recommended-Readings.md#HARKing-Hypothesizing-After-the-Results-are-Known)).

Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance. -->

Submitting work for *informative* review would also reduce the time authors need to wait for feedback, because most of the current delays is needed to compare competing submissions. In competitive peer review, authors cannot receive feedback until reviewers must review a set of competing submissions, discuss each submission with other reviewers, and collectively determine outcomes for all submissions. *Informative* reviews can be shared immediately after evaluating the work in isolation. We could create *informative* peer review committees for time-critical topics that could respond in days instead of months. We could dramatically accelerate the pace of scientific discourse.

Opting out of competitive peer review would save authors time and effort, and reviewers as well. It would  eliminate the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to a new peer review committee with different members, expectations, interests, and paper formatting rule. Even if work is unchanged, each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch.

Research could also make science more efficient by submitting work for *informative* peer review before running experiments, so that we can improve experimental methodologies prior to incurring the costs of the experiment.  This could be especially useful for human-subjects experiments and other experiments that are expensive to re-run, or that have a potential for harm each time they are run. Current practice is wasteful because requests to improve experimental methodology arrive long after experiments have been run.

Embracing *informative* peer review could also make science kinder and more inclusive. We could expect more constructive and positive feedback from *informative* peer review, at least in aggregate, because reviewers would not need to concoct reasons to justify rejecting the majority of papers. Informative peer review does not conflate reviewers' subjective disinterest in a work, or sense that it lacks significance, with a lack of scientific integrity. Offering peer review that is not competitive could make science more inclusive, as reviewers are naturally biased to accept work by authors similar to themselves, whether its because the work addresses problems similar to their own or whether its because the writing is in a dialect similar to their own. 

<!-- The predominance of negative feedback biases who becomes a scientist. The scientific community loses talented aspiring scientists who are uncomfortable promoting their work as important, or who are too "thin-skinned" or "insufficiently perseverant" to discard feedback that argues their work is unimportant. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback causes at least two selection biases in who survives the process of becoming a scientist.

First, it makes those more comfortable promoting the importance of their work, or whose sheer self confidence allows them to dismiss negative feedback, most likely survive. Those who believe others will be interested the research they've conducted, and are most able to ignore reviewers' disinterest, may also be the most able to ignore experimental results that refute their preconceptions. Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates” [[Smaldino and McElreath]](./Recommended-Readings.md/#the-natural-selection-of-bad-science). -->

---

*Competitive* peer review was once necessary due to financial constraints on how much could be published: when research was printed on paper, traveled by mail, and took up space on shelves. As publishing costs approached zero and the information available to us grew exponentially, we acquired new tools to filter the deluge of information competing for our attention, such as search engines and social sharing. Scientific progress is not advanced by forming committees curate which research is worthy of modeling on science's most exclusive runways[^runways] and which to leave hanging backstage. 

Whereas competitive peer review is the key building block supporting science's social stratification system, informative peer review is designed to be of no value to it. Exclusive publications can't use it decide which papers to accept. Conferences can't use it decide which papers are worthy of presentation slots. Those evaluating authors for jobs or awards associate acceptances with recognition if no acceptances are generated. For those averse to pitting our peers against against each other, *informative* peer review allows us to serve science without doing so.

Most of us may still want to present work at conferences and see others' work presented. Conferences use competitive peer review to allocates rooms and speaker slots to research work that reviewers believe attendees should want to see. Conferences could instead ask attendees which work they actually want to see. This would allow researchers to write work for their target audience, submit that work for *informative* peer review, and then present it to members of that target audience (and others) who opt to see it. Such a practice would remove a level of indirection that biases what gets presented to favor the interests of those senior enough to serve on review committees.

<!-- Some will welcome the opportunity to seek out scientific peer review, and review others' work, without becoming complicit in the toxicity of making research compete for recognition and prestige. -->
Some of us will still want to seek out the prestige that comes from presenting at an exclusive conference, publishing in an exclusive journal, or from other awards and recognition. Others may feel compelled to seek out such recognition by the expectations of co-authors or feel it's necessary for career advancement. We can still seek out *informative* review first. We could then revise our work and demonstrate that we had already responded to expert feedback before submitting for competitive review. 


Simply separating informative scientific review from competitive review would remove the pretense that competitive review serves science, when its true purpose is to distribute status to stratify researchers. Removing that pretense of service scientific integrity would help the scientific community better recognize, and reckon with, the costs of introducing zero-sum games into a cooperative endeavor meant to increase *everyone's* knowledge. It would make complicity in the toxicity of those games less of a necessity, and more of a choice.

<!-- When acceptance to journals is not equated with scientific integrity, journals are awards committees.
When communities have the opportunity to meet at conferences where attendees can , ...
Separate service to science from service to stratification. -->

<!-- Ranking is important for hiring at top universities  -->


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). Those who might dismiss Stuart's arguments by concluding that he is sort of mad scientist would be wise to consider an alternate hypothesis, that they may have only seen him mildly aggrieved. You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).

---

<!-- Move notes here?  -->

[^replicability]: Past attempts to test whether peer include the [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/), and its 2014 predecessor, which duplicated the review process for a fraction of papers. Per the 2021 report:
 
 > In 2014, 49.5% of the papers accepted by the first committee were rejected by the second (with a fairly wide confidence interval as the experiment included only 116 papers).  This year, this number was 50.6%.
 
 The experimental methodology is noteworthy for removing independent variables that might cause differences between the replica review processes. Again, in their words:
 
 > During the assignment phase of the review process, we chose 10% of papers uniformly at random—we’ll refer to these as the “duplicated papers.”  We assigned two Area Chairs (ACs) and twice the normal number of reviewers to these papers.  With the help and guidance of the team at OpenReview, we then created a copy of each of these papers and split the ACs and reviewers at random between the two copies.  We made sure that the two ACs were assigned to two different Senior Area Chairs (SACs) so that no SAC handled both copies of the same paper.  Any newly invited reviewer for one copy was automatically added as a conflict for the other copy.  We’ll refer to the SAC, AC, and reviewers assigned to the same copy as the copy’s “committee.”  The papers’ committees were not told about the experiment and were not aware the paper had been duplicated.
 
 This methodology assured that papers were reviewed to a single peer review committee's reviewing standards and with reviewers from the same pool of candidates (a reviewer might be reviewing a paper from replica A and a different paper for replica B).
 
 We can expect even lower reliability under less controlled circumstances, such as when authors re-submit their work to a different peer review committee. Re-submissions means work will be evaluated by a committee with traditions and expectations, different pools of reviewers, with new prior work released in the intervening time, and with changes in what research is fashionable over the passage of time.


[^runways]: Runways may be engineered hills, but they are still a poor choice for a scientist to want to die on.
