# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower us with others' expertise and insights to make us all better scientists and communicators.

But practices we call peer review can also hinder science instead of advancing it.

We hinder by pitting research works against each other to vie for inclusion in exclusive publications or for other forms of recognition. We practice this *competitive* peer review under the pretense of advancing science while picking winners using criteria that inevitably undermine scientific integrity.[^undermine-integrity] We believe we can evaluate these criteria fairly despite no evidence to back up this self confidence; in fact, in the few well-designed experiments of competitive peer review, [half of the works accepted by one review were rejected by a parallel review](./Recommended-Readings.md#the-neurips-2021-consistency-experiment).

Yet, even if we conducted peer review work non-competitively and for integrity alone, we could not objectively or fairly segregate works to accept and reject. Scientific integrity has many dimensions. Compressing them to a boolean is inherently subjective[^subjective-integrity], necessarily lossy, and thus likely to mislead.

---

Rejecting works with flaws is also not the only way, nor even the best way, to protect scientific integrity.

The allure of the *reject* is its prophylactic power to prevent flawed or misleading work from reaching its audience—they cannot be mislead by that which they cannot see. However, a *reject* does not cure a work's flaws and its protection is limited to a single submission. The rejected work's authors may still re-submit their work unchanged elsewhere or publish it without peer review.

Further, our power to reject has broader side effects that harm integrity. Our choice to *not* reject a work –  to accept it – is an implicit endorsement. An *accept* can lead the work's audience to overlook its limitations and shortcomings, which may be significant even if not crossing the threshold of rejection. When a *reject* does succeed in censoring works with significant missteps, it conceals those missteps from those who might study them or learn from them, making it harder for science to learn from our collective mistakes and immunize ourselves against them.

The power to reject work can also subvert our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that decision. We may unfairly ignore observations that support the alternative outcome, as such observations create dissonance with our faith in our ability to objectively choose a fair outcome.

The product of *scientific* peer review need not, and should not, be a decision to *accept* or *reject* a work. By abdicating the power to reject, we can avoid being corrupted by it, avoid incorrectly compressing integrity into a boolean, avoid picking winners unfairly, and avoid censoring our mistakes from those who would learn from them.

---

Peer review should instead produce evaluations written to *inform* the work's authors *and* audience. To ensure the audience can benefit from those evaluations, authors who submit for this *informative* form of peer review should be required to share the reviews if and when they publish their work (or otherwise share it with others).[^social-contract]

In such *informative*[^open-peer-review] peer review, reviewers could address their evaluations directly to the work's potential audience, to ensure that audience is not disadvantaged by lacking reviewers' expertise, or by lacking the time to read as diligently as reviewers are expected to. For deeply-flawed work, reviewers could make their concerns apparent to the work's audience, signalling a lack of credence equivalent to that which would be conveyed by a *reject*[^rejects-invisible-if-unpublished] (or even exceeding that lack of credence). When reviewing particularly meticulous work, reviewers may illuminate how the work exceeded standards expected for a mere *accept*. Whereas peer review that concludes with a *reject* only protects a work's audience from being mislead by exposure to the reviewed work, *informative* peer review could help to immunize consumers of science against other misleading content.

This skills specific to *informative* peer review, elucidating insights to inform a broad audience, are those we should expect fellow scientists to possess and be comfortable using. Our skills as scientific collaborators can also help us persuade authors to improve their work: explaining the purpose and value of proposed improvements, providing constructive feedback to make improvements easier to realize, and offering the promise to update our reviews to acknowledge genuine improvements. 

Some authors will inevitably still receive feedback they dislike, disagree with, and even feel the feedback misleads their audience. While authors would be obligated to share that feedback, they could rebut that which they disagree with, encourage other reviewers to address the disagreement, or even invite outside experts to do so.

---

Increasing the objectivity of scientific peer review is just one of the many benefits of adopting *informative* peer review.

Submitting work for *informative* review would empower authors to write for their intended audience, not to court acceptance from reviewers and minimize their risk of rejection. Seasoned researchers preparing work for competitive peer review write to avoid rejection without even thinking about it, often not realizing how differently we might write were it not necessary to do so. We open our papers with introductions written to convince disinterested reviewers that our work is worthy of their interest, not to help potential readers decide whether our work is what they are looking for. We cite superfluous works by potential reviewers, use unnecessary jargon coined by potential reviewers, and employ frameworks invented by potential reviewers, all to signal that we are experts in the field who appreciate their contributions and to humble ourselves before them. Our exploding expectations for citation counts when reviewing work, and our need to exceed those expectations when submitting work, have caused the bibliographies we produce to evolve like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

Submitting work for *informative* review would also reduce the time authors need to wait for feedback, because the bulk of time needed to compare competing submissions. Reviewers must evaluate all their assigned submissions and discuss them with other reviewers. A reviewer conducting an *informative* review need only read one work and may choose to share feedback without awaiting others' reviews. Adopting *informative* peer review could dramatically accelerate the pace of scientific discourse, allowing time-critical work to receive feedback in days instead of months.

Researchers could also make science more efficient by submitting their experiments for *informative* peer review, and receiving feedback, before running those experiments or collecting any data. This could be especially beneficial for improving human-subjects experiments and other experiments that are expensive to re-run, or that have a potential for harm each time they are run. The current practice of seeking review after incurring an experiments costs and risks is wasteful.

Opting out of competitive peer review would save authors and reviewers time and effort. It would eliminate the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to appeal to a new peer review committee with different members, expectations, interests, and paper formatting rules. Each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch, even if work is effectively unchanged from submission to prior reviewers.

Embracing *informative* peer review could also make science kinder. When peer review is exclusively *informative* reviewers can write for the benefit of the work's potential audience, not to convince fellow reviewers to agree on an acceptance decision, nor to convince authors that a decision is fair. We could expect more constructive and positive feedback from *informative* peer review, at least in aggregate, because reviewers would not need to concoct reasons to justify rejecting the majority of papers. It allows researchers to get feedback on competently-executed work that would likely be rejected for being uninteresting or unfashionable—such as replication studies.

Offering peer review that is not competitive could make science more inclusive, as reviewers are naturally biased to accept work by authors similar to themselves, whether its because the work addresses problems similar to their own or whether its because the writing uses a dialect similar to their own. We would also lose fewer aspiring scientists who are uncomfortable promoting their work as important, or who are too “thin-skinned” or ”“insufficiently perseverant” to discard feedback that argues their work is unimportant. While the ability to overcome challenges is an asset in science and most professions, expecting persistence in the face of negative feedback biases who survives the process of becoming a scientist in favor those best able to ignore evidence.[^selection-of-scientists] Competitive peer review drives out those who see zero sum games as antithetical to an endeavor meant to increase *everyone's* knowledge.

Embracing *informative* peer review would also make science more transparent. Whether reviewers remain anonymous or not, abdicating the power to reject work would help prevent the abuses that result from that power being wielded anonymously.[^anonymity]

---

Providing opportunities to request and contribute to *informative* peer review in our fields is an important first step to escaping the monoculture of competitive peer review.

Competitive peer review was once necessary to restrict how much could be published and stored due to financial constraints: when research was printed on paper, traveled by mail, and took up space on shelves. It no longer is. Curation is also less necessary as we have evolved tools to filter the deluge of information competing for our attention, such as search engines and social sharing.

Nor is *competitive* review needed for events such as conferences. We could run conferences which schedule talks based on the work attendees ask to see, instead of what a biased[^reviewers-are-biased-sample] sample of attendees (disproportionately senior and powerful) think they should want to see.

Yet, for the forseeable future, the evaluation systems used to rank us and measure our progress will demand that we, or our collaborators, submit work for competitive peer review. We can do so subsequent to *informative* peer review, giving competitive reviewers access to our informative reviews.

Every time we request or contribute to *informative* peer review can take a step to upending the pretense that *competitive* review advances science.
<!-- It would help help the scientific community better recognize the trade-offs of competitive review. It would help us reckon with competitive peer review's role as the most important building block of science's social stratification system. -->


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^undermine-integrity]: Rejecting work that isn't ‘*significant*’ encourages authors to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: *Informative* peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of competitive peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of *informative* peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.

[^rejects-invisible-if-unpublished]: The originally-intended audience of a work may not not see the reviews if authors choose not to publish it.

[^anonymity]: Informative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates” [[Smaldino and McElreath]](./Recommended-Readings.md/#the-natural-selection-of-bad-science).


[^reviewers-are-biased-sample]: Reviewers are disproportionately older and more powerful than the general population of conference attendees. We should be suspect of those who believe that they are best able to choose which work should be modeled on science's more exclusive runways and which to leave hanging backstage. Runways may be engineered hills, but they are still poor hills for scientists to choose to die on.
