<!-- Exclusionary, reductive, decisive  -->
<!-- # Peer Review Should *Elucidate* not *Exclude*. -->
<!-- # Peer Review Should Illuminate not Exclude -->
<!-- # Rejecting Exclusionary Peer Review -->
<!-- # Power, Prestige, and Peer Review -->
<!-- # Peer Review in Service to Science -->
<!-- # Informative Peer Review -->
# Rejecting the use of *Reject* in Scientific Peer Review
<!-- The fundamental flaw in our practice of scientific peer review is building it around the decision of whether work should be accepted or rejected. -->
<!-- Peer review can be assistive or decisive, not both. -->
<!-- Industrial labor -->

Peer review currently serves competing purposes, and should be split into:

1. *Scientific* peer review, to protect scientific integrity, which should not produce *accept* and *reject* outcomes
1. Peer review, to identify whether research works are worthy of publication or other recognition, which should not purport to protect scientific integrity,

Scientific peer review should not produce *accepts* and *rejects*. When evaluating whether research works are worthy of publication or other recognition, we should abandon the pretense that such review serves to protect scientific integrity.


Segregating research works into *accepts* and *rejects* undermines scientific integrity and scientific discourse.

We should isolate *scientific* peer review from reviews conducted to decide which works are, and are not, worthy of publication or other recognition.

Surrendering the power to reject deeply-flawed work could harm to scientific integrity, allowing the work to mislead those who do not read the reviews or do not understand them. Were we concerned about the integrity of a single work, and not the integrity of science in the aggregate, a *reject* might be optimal. But the power to *reject* some works means that we will *accept* others. Those *accepts* will be misconstrued as endorsements, even though accepted works are rarely without limitations and shortcomings that many readers will overlook. When we reject work to prevent one dubious result from misleading readers, we censor the entirety of the work, including results that may be credible and valuable. Rejecting work censors the record of authors' missteps, making it harder for science to learn the prevalence of those missteps or to learn from them. While luck may permit the credibility of science to increase when we hide one mistake, in aggregate the practice of hiding missteps is harmful.


That we can protect scientific integrity without the power to *reject* work may be surprising to those who have 

When reviewing a single work that has severe lacks credibility, the option to *reject* may seem like the surest way to protect scientific integrity.    The power to *reject* work seems like the surest way to protect those who might be mislead by it, but

The power to *reject* work is that censoring work seems like the surest way to prevent it from misleading others. But once we have *rejects* and *accepts*, the latter will be misconstrued as endorsements, even though accepted works are rarely without limitations and shortcomings that many readers will overlook. When we reject work to prevent one dubious result from misleading readers, we censor the entirety of the work, including results that may be credible and valuable. Rejecting work censors the record of authors' missteps, making it harder for science to learn the prevalence of those missteps or to learn from them.

The current practice of peer review purports to serve scientific integrity while also identifying works of ‘*significance*’ worthy of publication. Yet, making work appear significant can compete with [integrity](./The-Harms-of-Exclusionary-Peer-Reivew.md). Current practice perpetuates the illusion that scientists can measure ‘*significance*’ reliably and reproducibly, when in the few well-designed experiments evaluating that hypothesis, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). 




We can properly measure the cost/benefit tradeoff of recognizing work of importance if we isolate it from the pretense of integrity.




We should conduct scientific peer review to advance the integrity and quality of scientific discourse, without producing *accept* or *reject* outcomes. Evaluating whether work is worthy of publication or other recognition serves other, often conflicting, goals and so scientific review should be performed independently of such review.


For scientific peer review to advance the integrity and quality of scientific discourse it need not, and should not, produce *accept* or *reject* outcomes. Scientific review should not evaluate whether research work is worthy of publication or other recognition.





We cannot accept works with integrity and reject works without it because integrity is neither binary nor objective. It is multidimensional because there are myriad causes to questions a work's findings, and there are as many trade-offs in designing research depending on one's subjective concern with each potential cause. Work describing an experiment may fairly be a *reject* to a reviewer who notes that the experimental design failed to disprove a plausible alternate hypothesis, but an *accept* to a reviewer who considers that alternate hypothesis too implausible to warrant disproving.


Scientific integrity multidimensional and , Reducing the We cannot protect scientific integrity by Producing *accepts* and *rejects* because integrity is multidimensional, not binary, and  

When reviewing peers' research work to determine if it is worthy of appearing in an exclusive publication or receiving other recognition, we should not do so under the pretense protecting scientific integrity. This form of review requires us to evaluate criteria of ‘*significance*’ that extend beyond scientific integrity, which can also also compete with integrity and [undermine it](./The-Harms-of-Exclusionary-Peer-Reivew.md). It also perpetuates the illusion that scientists can measure ‘*significance*’ reliably and reproducibly, when in the few well-designed experiments evaluating that hypothesis, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). 





 is incompatible with these goals, as is attempting to compressing the integrity of a work into a binary judgement such as *accept* or *reject*.

 often conflate this purpose with the act of deciding what research is worth publishing when these purposes actually conflict.



For science to be objective and credible, we must perform peer review that does not produce *accept* or *reject* outcomes, nor decide whether or where work will be published.

Deciding whether to accept or reject research work for publication is incompatible with *scientific* peer review.

Peer reviewing with the power to reject others' work is fundamentally incompatible service to science is fundamentally incompatible 

1. We simultaneously review for scientific integrity and ‘*significance*’, when these criteria are fundamentally incompatible.
2. We conduct peer review to produce a binary outcome (*accept* or *reject*) though when that reduction is unnecessary and harmful.

Therefore

1. We should conduct *scientific* peer review without producing *accept* or *reject* outcomes, instead requiring authors to publish reviews with their work.
2. We should differentiate scientific peer review from rituals that generate *accepts* and *rejects*, especially that which seeks to identify works of significance. We should not participate in such rituals under the pretense of serving scientific integrity.


Peer reviewing
We do not serve science when we peer reviewing research to determine whether to *accept* it for publication or *reject* it.

Peer review can serve science, or peer review can decide which work should or should not be published, but it cannot do both.

 *accept* or *reject* from publication, are   whether research is worthy of publication are two distinct and incompatible, and the former must be 

Truly scientific peer review should not decide where or whether research work is published, nor reduce reviewers' insights into a binary *accept* or *reject*.


Most of volunteer to review peers' research aspiring to *advance* science. We seek to protect the integrity of scientific research by revealing missteps, errors, and other problems that authors might have overlooked and that might mislead the work's audience. We hope to provide feedback that improves authors' work, helping them to become better scientists and communicators.

But peer review that requires us to help decide to *accept* or *reject* works *hinders* science under the pretense of advancing it.


Instead, we should peer review on the condition that authors share our reviews along with their work when they publish it. We can write our reviews to ensure the work's audience is not disadvantaged by lacking expertise reviewers should have or the time to read as diligently as reviewers are expected to. We can ensure the audience is aware of grave concerns that would have led to certain rejection were the option available, raves that would have inspired enthusiastic acceptance, and all the less clear cut  observations but important observations in between.

The appeal of being able to *reject* work is that censoring work seems like the surest way to prevent it from misleading others. But once we have *rejects* and *accepts*, the latter will be misconstrued as endorsements, even though accepted works are rarely without limitations and shortcomings that many readers will overlook. When we reject work to prevent one dubious result from misleading readers, we censor the entirety of the work, including results that may be credible and valuable. Rejecting work censors the record of authors' missteps, making it harder for science to learn the prevalence of those missteps or to learn from them.


Reducing our review of a work to an *accept* or *reject* is unscientific because we cannot do so reliably; in the few well-designed experiments evaluating peer review, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability).

One might argue that we are unreliable reviewers because we are asked to evaluate not just the integrity of work, but also its subjective ‘*significance*’. Yet, integrity is not without subjectivity. Work describing an experiment may fairly be classified *reject* by a reviewer concerned that the experimental design failed to disprove a plausible alternate hypothesis, but an *accept* to a reviewer who believes that alternate hypothesis is too implausible to warrant disproving.

When reviews are reduced to *accepts* and *rejects, acceptances are inevitably misconstrued as endorsements, though accepted works are rarely without limitations and shortcomings that many readers will overlook. When we reject work to prevent one dubious result from misleading readers, we censor the entirety of the work, including results that may be credible and valuable. Further, when reject work, we are censoring the record of authors' missteps, making it harder for science to learn the prevalence of those missteps or to learn from them.

Holding power over whether a work will published reduces our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that outcome. We may be unfairly ignore observations that support the alternative, as they create dissonance with our belief that we chose the fair objective outcome.





 <!-- We practice exclusionary peer review because of the collective illusion that is is somehow necessary for science. -->

Exclusive publications often ask us to evaluate criteria of ‘*significance*’ that extend beyond scientific integrity, when these added criteria can compete with integrity and [undermine it](./The-Harms-of-Exclusionary-Peer-Reivew.md). Reviewing work for ‘*significance*’ risks perpetuating the illusion that scientists can measure ‘*significance*’ reliably and reproducibly, when in the few well-designed experiments evaluating that hypothesis, [half of the works accepted by one review were rejected by a parallel review](./Notes.md#replicability). 

Yet, even were to to evaluate research for scientific integrity alone, we would be unable to objectively or accurately compress the many dimensions of integrity into *accepts* and *rejects*. Work describing an experiment may fairly be a *reject* to a reviewer who notes that the experimental design failed to disprove a plausible alternate hypothesis, but an *accept* to a reviewer who considers that alternate hypothesis too implausible to warrant disproving.

One consequence of this lossy compression is that acceptances are inevitably misconstrued as endorsements, though accepted works are rarely without limitations and shortcomings that many readers will overlook. When we reject work to prevent one dubious result from misleading readers, we censor the entirety of the work, including results that may be credible and valuable. Further, when reject work, we are censoring the record of authors' missteps, making it harder for science to learn the prevalence of those missteps or to learn from them.

Holding power over whether a work will published reduces our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that outcome. We may be unfairly ignore observations that support the alternative, as they create dissonance with our belief that we chose the fair objective outcome.
<!-- (Ironically, the criminal justice system attempts to address such biases by separating the roles of investigator and prosecutor, whereas scientists who actually study cognitive biases do not take equivalent precautions to remove their biases when judging peers' work.) -->

To best advance science, we should abdicate the power to decide whether or where that research is published. Power cannot corrupt us if we decline to wield it. Labels of *accept* and *reject* cannot mislead others if we decline to produce them.


We can protect scientific integrity by reviewing work under the condition that authors share our reviews along with their work when they publish it. <!-- We can protect scientific integrity by reviewing work under the condition that authors share our reviews along with their work when they publish it. 
--> We can write reviews that illuminate our concerns and other insights for the works' audience, ensuring the audience is not disadvantaged by lacking expertise or the time to read as diligently as we are expected to when reviewing. For work with missteps, errors, or concerns so grave that we would have once rejected it, we can make these concerns, and our lack of credence, as apparent to the work's audience as a *reject* might have been (though that audience may not see our reviews if authors choose not to publish, as it might not know work had previously received a *reject*). When reviewing particularly meticulous work, we can help the audience understand why we find it more credible than a mere *accept* might imply.

Nor do we need the power to reject research to encourage authors to improve it. We can strive to persuade authors that revisions will improve their work, to provide constructive feedback to reduce the effort needed to improve the work, and promise to update our reviews to acknowledge those improvements.

Replacing *accepts* and *rejects* with *informative* peer review would make scientific discourse more **credible**, more **understandable**, **faster**, more **efficient**, more **inclusive**, **kinder**.

More credible: no more perverse incentives.
More understandable: correct audience.
Faster: no waiting to compare
Efficient: no more reject and resubmit (submit before experiment)
Inclusive: scientific feedback without rejection for outsider status
Kinder: rejection only for status

<!-- This *elucidative* peer review serves scientific integrity because it removes perverse incentives that encourage authors to downplay missteps or overstate *significance* to avoid rejection. It also would make scientific discourse more **understandable**, **efficient**, **faster**, **kinder**, more **inclusive**. -->



Were we accustomed to *elucidative* peer review, we would expect scientific discourse to progress at a much faster rate than it can today. We would be accustomed to results being published after rounds of feedback between authors and a single set of reviewers. Adopting *exclusionary* peer review adds 3-6 months for every cycle of rejection and re-submission.

<!-- Given how invested most of us are in the tradition of *exclusionary* peer review, and the journals, conferences, hiring systems, and promotions systems built around this ritual, it can be difficult to imagine adopting a peer review custom that does not give us the power to reject others' work and does not confer prestige as part of evaluating scientific integrity. So, before discussing the path to adopting *elucidative* peer review, consider what we would think of *exclusionary* peer review if *elucidative* reviewer were our  current custom.
 -->




New results would be delayed for years while work is rejected and re-submitted over and over. 

 be balk at how *exclusionary* peer review would slow the speed of scientific discourse. We would ask how science is served by adopting a cycle in which we must re-submit rejected papers to new peer reviewers, over and over. We would not be accustomed to long waits between submitting our work and getting feedback, because reviewers wouldn't need to compare research to competing works, or debate its worthiness with other reviewers, before sharing it with us. We would be accustomed to getting feedback while our [memories are fresher](./The-Harms-of-Exclusionary-Peer-Reivew.md#delays-disrupt-memory), when we can best act on it.

<!-- If we didn't have to write reviews to support an *accept* or *reject* decision, we would less likely to bias ourselves in selecting the points salient to include in our reviews. -->

<!-- even if authors fail to address them, we could -->


<!-- that science is advanced by giving peer reviewers power to *exclude* others' work from publication is a collective illusion.

If our goal is to ensure that missteps, errors, and other problems don't go undisclosed, we should not reject papers for any shortcomings disclosed by authors—incentives for authors to hide problems subvert scientific integrity. In fact, we should reconsider whether, when we review our peers' work, we should hold the power to decide if it should be published. It is not the only way for peer reviewers to ensure problems don't go undisclosed. -->

<!-- If we required authors to publish peer reviews along with their work, we could write reviews that elucidate our concerns in language accessible to the audience of the research.

Such *elucidative* peer review would require participating authors to share those reviews when they publish their work (see [notes](./Notes.md#publishing-reviews) for details). -->



Eliminating the cycle of rejection and re-submission would also save time and effort for both authors and reviewers. Diligent authors may take great pains to revise work for re-submission to a new peer review committee with different members, expectations, interests, and paper formatting rule. Even if work is unchanged, each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch. Authors should still incorporate feedback nad improve their work, but reviewers who have already read the work and provided that feedback can evaluate those changes more efficiently than new ones.

But beyond being faster, and saving effort, *elucidative* peer review would change the audience that both authors and reviewers write for, to everyone's benefit. In *exclusionary* peer review, authors write to convince reviewers their work is [worthy of acceptance](./The-Harms-of-Exclusionary-Peer-Reivew.md#audience), and reviewers write to convince other reviewers to either accept or reject. *Elucidative* peer review would free authors to write for their true audience, and give reviewers the opportunity to write for a larger audience as well.

We could expect more constructive and positive feedback from *elucidative* peer review, at least in aggregate, because reviewers would not need to [concoct reasons to reject the majority of papers](./The-Harms-of-Exclusionary-Peer-Reivew.md/#negative-feedback).

We would still get negative feedback, some of which we may disagree with. We would be obligated to share feedback with disagree with along with our work. We could, however, rebut feedback we disagree with and publish, allowing the broader scientific community to evaluate the merits of each position.


<!-- Authors write to persuade reviewers their work is worthy publishing, which may not serve their [true audience](./The-Harms-of-Exclusionary-Peer-Reivew.md#audience). Reviewers write to justify for the score or outcome they will advocate for when discussing the paper with other reviewers. 
 -->

<!-- We can publish whenever we have addressed reviewers' concerns to our satisfaction. (Whether reviewers or the audience are satisfied is for them to decide.) -->

<!-- So as to focus on the benefits of *elucidative* peer review, I document the [harms of *exclusionary* peer review]() separately and only allude to them here. I will explain why elucidative peer review benefits us all whether we are consuming research, authoring research, reviewing research, as well as participation in science and broader trust in science. -->

## How we benefit, as authors, when publishing research

<!-- <a id="faster"></a>*Elucidative* peer review is [faster than *exclusionary* peer review](./The-Harms-of-Exclusionary-Peer-Reivew.md#delaying-the-spread-of-knowledge). When we are ready to submit work, we do not need for a deadline when others will be submitting competing work, and reviewers' feedback need not be withheld until our work can be compared with others' work, or for reviewers' feedback compared with others' reviews. This means we can act on feedback sooner, while [our memories are fresher](./The-Harms-of-Exclusionary-Peer-Reivew.md#delays-disrupt-memory). We can publish whenever we have addressed reviewers' concerns to our satisfaction. (Whether reviewers or the audience are satisfied is for them to decide.) -->

<a id="research-audience"></a>When preparing work for elucidative peer review, we have the latitude to write to best serve our audience. When we write to defend against rejection, we make our work [less readable](./The-Harms-of-Exclusionary-Peer-Reivew.md#audience).

<!-- as opposed to minimizing our risk of rejection. Defensive writing encourages us to fawn over tangentially-related  work written by likely reviewers from our subfield, regardless of that work's quality or true relevance; to dedicate paragraphs to promoting the importance of our work to reviewers whose interests took them to different subfields; and to bulk up citation counts to signal our domain expertise to all reviewers. -->



## How we benefit as reviewers of others' research

Whereas exclusionary peer review demands that scientists pass final judgements, *elucidative* peer review only requires them to use the skills already essential to them as authors: examining others' research and explaining it to others.

When reviewing, we can not only elucidate concerns to share with those who might learn about the work (as journalists are limited to), but we can attempt to persuade authors to revise their work to address those concerns. It is better for authors to eliminate our causes for concern, or disclose those concerns on their own, than for the audience to only learn about the concerns from our reviews. (When authors address our concerns, we can remove those concerns from our reviews or note if we are satisfied with authors' changes.)

participating in elucidative peer review allows us to perform a service to authors, and to serve science more broadly, without being complicit in ranking and exclusion. Elucidative reviewing gives our reviews a higher purpose and broader audience than exclusionary reviewing. It also lightens our reviewing workloads, as authors need not resubmit their work repeatedly in search of reviewers who consider their work interesting enough to publish—each work needs only one set of reviewers.


## How we benefit as consumers of research

As consumers of research, elucidative peer review provides far more insight than learning only that a paper was accepted (or, for rejected papers, not even learning they ever existed). Like exclusionary [open peer review](FAQ.md#open-peer-review), we in the audience are privy to reviewers' feedback.

When authors opt for elucidative peer review, we consumers get access to peer-reviewed research and that's [written for us](#research-audience), not other reviewers, and we get access to it [faster](./Notes.md#faster). When reviewers do not have to write their reviews to support a position (*accept* or *reject*), they can be more objective. When they don't have to convince other reviewers to join their position, they can direct their insights to us, the work's full audience, as opposed to other experts.

We should also demand more control over the decisions that guide our access to research that interests us, rather than what others think we should find worth reading. For example, we could eschew conferences where an elite group of peer reviewers decide what which presentations we want to see, and instead opt for those that ask us which prospective presentations we would want to see and allocate speaker slots and lecture hall sizes to give the audience what we're actually interested in.


#### We can free ourselves from outcome bias

However, when reviewers have to make a decision to accept or reject research, and start favoring one of those two options during the review process, they may consciously or subconsciously start biasing their collection and presentation of evidence to support the position they expect their review to take. Only by eliminating the decision entirely can we remove this bias.

Also, the choice to ‘*reject*’ research implies that there is also a decision to ‘*accept*’ it. Whenever we ‘*accept*’ research we risk that this act will be perceived as scientific consensus on correctness, when in fact it would only mean that none of a small number of experts found sufficient reason to reject it. Science takes a long time to build consensus around our confidence in results. When the public prematurely concludes finality and correctness, then sees conflicting results later also endorsed with an ‘*accept*’, they may lose faith in science.

#### We can free ourselves from reviewing work others have already provided sufficient feedback on
Having countless exclusionary peer review committees of different strata also burdens us with unnecessary work as reviewers. We must re-review papers that had already met scientific-integrity requirements when previously rejected. Our efforts to provide constructive feedback are discarded by authors who assume that what their work really needs is a new set of reviewers with different subjective expectations.

#### We don't have to participate in the culture of rejection




## Benefits for science



#### The body of published results will be less biased
**Excluding research biases the body of published results**. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences of *publication bias* the potential for false discovery, especially when peer review committees deem null results unworthy of publication. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective (at least at p<0.05)—and get published.

#### Peer review will mean what we say it means
**Stratification built on exclusionary peer review undermines scientific standards**. Every journal and conference has different standards of acceptance. Many journals and conferences publish work they know to be flawed to meet financial or attendance goals. The lowest strata publications may provide the pretense of peer review while accepting any paper for which authors pay publication fees. Even the most prestigious conferences in my field are known to override some reviewer's integrity concerns if others believe the work will draw attention that might otherwise go to competing conferences. Yet, many outsiders are unfamiliar with these different standards, or are unaware that scientific standards vary so broadly depending on the publication.

<!-- --- -->
#### We will broaden and improve participation in science
[biasing](./The-Harms-of-Exclusionary-Peer-Reivew.md#biasing-the-demographics-of-science)


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). Those who might dismiss Stuart's arguments by concluding that he is sort of mad scientist would be wise to consider an alternate hypothesis, that they may have only seen him mildly aggrieved. You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).

---

[Acknowledgements](./Acknowledgements.md) | [FAQ](./FAQ.md) | [Notes](./Notes.md) | [Recommended readings](Recommended-Readings.md)
<!-- em — , en –   …  -->
