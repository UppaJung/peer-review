## Peer Review as we Say, Not as we Do

We scientists say we practice peer review to protect the scientific integrity of research: employing objective experts to identify its flaws, evaluate its credibility, and ensure it does not confuse or mislead. Since we have convinced the press and public that peer review is a signal that research is credible, we *should* make paramount the goal of preventing research from misleading.

But in much of science, what we are actually doing when we peer review research is to rank it relative to other research competing for attention and, by extension, to rank our fellow scientists relative to each other. Peer review committees are run by journals and conferences, each of which has its own brand, and which signal brand prestige by being selective in accepting submissions. They use reviewers as gatekeepers to determine which research will be of interest to their audience.<!-- Our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on peer review committees. --> Being selective often means rejecting some submissions of sufficient integrity that lack such subjective qualities such as "novelty", "impact", or that particularly nebulous catch-all (really a *drop-all*) of a "significant contribution". As scientists, our ability to hurl our research over these gates determines our career trajectory in science's stratification system. 

There are myriad ways in which our using peer review to determine what is worthy of publication undermines scientific writing, and is harmful to scientists and to science itself, especially when we use subjective measures that may not align with scientific integrity. So when we claim peer review protects scientific integrity, supposedly preventing scientists from misleading ourselves and the public, we are mischaracterizing our current practice of peer review in a way that actually misleads ourselves and the public.

Perversely, we scientists continue to make gatekeeping the overriding purpose of peer review even though this function has become almost entirely vestigial to science. There were once financial forces that made information exchange expensive and constrained how much could be published: when research was printed on paper, traveled by mail, and took up space in libraries. In an era where publication is effectively free we no longer need peer reviewers to determine what is worth publishing. As the information available to us has grown exponentially, and we have faced a deluge of information competing for our attention, we acquired new tools to filter it, such as search engines and social media sharing. We no longer need peer reviewers to be arbiters of fashion, curating which research is eye-catching enough to model on their metaphorical runways and leaving the rest hanging backstage in science's sweaty dressing rooms.
<!-- Perhaps for the slide version of this talk "Runways are an odd slightly-raised surface for scientists to choose to die on." or "When I ask fellow scientists whether they really want to be arbiters of fashion, I'm often surprised how many will choose a runway as the the hill they're willing to die on." -->

In this article I will detail why gatekeeping what gets published is antithetical to protecting scientific integrity, examine what we can learn from the integrity protections in other social systems for establishing shared beliefs, and propose how we might apply those lessons to make peer review better serve science.

#### Peer review for gatekeeping harms science and scientists
There are many ways using peer review to rank research, and determine which research is interesting enough to publish, harms scientific writing and causes science to mislead ourselves and others.

**Gatekeeping biases the body of published results**. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

**Favoring interesting research causes authors to write less scientifically**. It inspires authors to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. It provokes them to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. It tempts authors to deceive others, and even themselves, into believing that hypothesis tests they conducted on data they found interesting were planned before they had seen the data. It motivates authors to aggrandize their research to look more important. Many publications even explicitly ask that authors go beyond factual reporting of results to speculate about their research's impact and importance.

**Making reviewers gatekeepers changes the audience research is written for**. When authors must craft papers to be accepted by reviewers they must make sacrifices in serving their intended audience. For example, as authors we often add volumes of citations of no value to anyone but our reviewers. We do so to signal to reviewers that we are experts in the field and to reduce the chance of being faulted for failing to cite a reviewer's favorite work. Since reviewers expect papers to have at least as many citations as previously-accepted work, we add even more unnecessary citations when writing on a relatively new topic for which there is little related work. As authors write to conform, not inform, papers have grown to meet reviewers' ever increasing expectations for citation counts, and with cited work receiving less explanation of why its relevant to help the non-expert reader. We also conform by maximizing the amount of information that can fit of a page limit (to maximize the "contribution" needed for acceptance), and using our full page limit, rather than publishing shorter (or longer) papers.

**Gatekeeping delays the spread of knowledge** because comparing competing submissions takes time: authors must often wait to submit their research until a common deadline; reviewers need time to read a cohort of submissions to compare them against each other; committees need time to collectively discuss which papers to accept. These delays can add months to each submission cycle. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish.

**Gatekeeping-induced delays disrupt authors' work and degrade our memories**. Time decays our ability to recall how we conducted our experiments, our observations during the experiments, and the tooling we used to process data and document our results. By the time have access to reviewers' feedback identifying what we failed to document, we may have already forgotten details that reviewers noticed we had elided. Coming back up to speed on that which we do remember also takes time and effort. Unless we are lucky enough to have our work accepted on its first submission, we must disrupt our other work to revise our submission to target a different publication, with different reviewers, with different interests, paper formatting rules, and other expectations.

**Stratification built on gated peer review undermines scientific standards**. Every journal and conference has different standards of acceptance. Many journals and conferences publish work they know to be flawed to meet financial or attendance goals. The lowest strata publications may provide the pretense of peer review while accepting any paper for which authors pay publication fees. Even the most prestigious conferences in my field are known to override some reviewer's integrity concerns if others believe the work will draw interest and attention that might otherwise go to competing conferences. Yet, many outsiders are unfamiliar with these different standards, or unaware that scientific standards vary so broadly depending on the publication.

<!-- --- -->
<!-- **Co-opting peer review to rank research has broader harms.** -->
<!-- #### 2. Gatekeeping research is harmful to scientists and science itself -->

**Peer reviewing for gatekeeping causes broader harms to science**.

Conferences and journals that reject a majority of papers will necessarily provide more negative feedback to the community than positive feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. 

The predominance of negative feedback biases who becomes a scientist. The scientific community loses talented aspiring scientists who are uncomfortable promoting their work as important, or who are too "thin-skinned" or "insufficiently perseverant" to discard feedback that conveys their work is uninteresting and unimportant. Those more comfortable promoting the importance of their work or whose sheer self confidence allows them to dismiss negative feedback survive.

Having countless peer review committees of different strata is also burdens us with unnecessary work as reviewers. We must re-review papers that had already met scientific-integrity requirements when previously rejected. Our efforts to provide constructive feedback are discarded by authors who assume what their work really needs is a new set of reviewers with different subjective expectations.

Being part of a system that produces toxic feedback for authors is also toxic to us as reviewers. We review alongside those who abuse the subjectivity and anonymity of peer review. We witness the trauma suffered by recipients of toxic feedback while we are obligated to protect those creating the toxins.
<!-- 
If only we could perform our scientific duties without participating in a stratification system toxic to science! -->

Without alternatives to gated peer review, we each must choose between shirking our scientific duty to review others' work or becoming ever more complicit in the system that exposes authors to unnecessarily toxic feedback.

<!-- --- -->

<!-- ### Part 2. Call to Action -->

#### Other systems for establishing shared beliefs

Peer review is part of a larger system of rituals that scientists use to build consensus around beliefs. In revaluating peer review, it is worth stepping back to evaluate how other systems establish consensus beliefs.

Justice systems investigate and seek to test hypotheses where lives hang in the balance: did someone ("a suspect") commit a criminal offense for which they should be punished. As anyone familiar with procedural television dramas knows, the criminal justice system separates the roles of investigating crimes from those who prosecute the offenders. The system's designers envisioned that those who choose to investigate a suspect might be biased to assume that hypothesis is correct, and so someone more objective should evaluate the collected evidence to decide whether it is strong enough to support prosecuting the hypothesis. Then, the prosecutor must make the case to a panel of members of the public, a jury, who final judgement.

From the perspective of those in criminal justice – with all its flaws and biases – how corrupt science must seem; first, for allowing researchers to both investigate and prosecute a hypothesis; and second, for placing final judgement in the hands of anonymous investigators, some of whom may be prosecuting (or have prosecuted) similar or competing hypotheses.

If we learned from the criminal just system, we would not encourage scientists to draw judgments beyond that which can be inferred by the reader from the evidence presented. We would also try to condense the opinions of peer reviewers into a boolean judgments of the validity of a work and treat acceptance as scientific consensus that the work is credible.

Journalists are also investigators who face decisions about what facts to treat as shared truths and who must reckon with their larger role of establishing shared beliefs. Those journalists who strive for impartiality try to let the observations they report speak for themselves.

For a model of how we might prevent research from misleading without gatekeeping what gets published, we might look to how the most diligent journalists report on new scientific research: they reach out to multiple objective scientific experts, ask those experts to speak to the credibility of research and the conclusions that might be drawn from it, and quote those experts so that readers know exactly what they said. When experts use language that may be unfamiliar to a broader audience, the best journalists help make it understandable to that audience.

The model of investigative journalism is *elucidative*: it informs the audience about observed disagreements rather than declaring the winner.

#### Like scientists, peer review should investigate and elucidate

Our practice of submitting research to publications that use peer review for gatekeeping, and reviewing for these publications, demands that we scientists be investigators, prosecutors, and jurors. The latter two roles should give us pause. If science is to be objective and credible, we must be investigators first.

The better we design and elucidate our scientific methodology and results as investigators, the better we enable others to draw accurate conclusions and build a shared consensus around them.

When conducting research as scientific investigators, we should be wary of any obligation to prosecute conclusions, especially those that promote the significance or importance of our results. When acting as peer reviewers to investigate others' work for errors, misleading statements, or other shortcomings, we should also be wary of any obligation to go beyond elucidating our findings, such as drawing conclusions about whether the work is important or significant.

To best serve science, we should thus conduct purely *elucidative* peer review, rather than drawing conclusions about whether work should be published. In elucidative peer review, we should address our concerns about how the research might mislead or confuse to the audience of the research---those who we want to protect from being mislead or confused. Our reviews should be published in tandem with the research to accompany it. In other words, our reviews should be *accessible* to the audience of the research both technologically and linguistically---the audience should be permitted to retrieve the reviews and should be able to understand them.

In conducting elucidative peer review, we scientists can go beyond elucidating for the audience, as journalists do, by also persuading authors to revise their work. The audience is best served if authors can to eliminate our causes for concern or, when that's not possible, to disclose those concerns on their own. When persuasion works and authors successfully address our concerns, we can revise our audience-directed feedback to remove our former concerns or note that we are satisfied with how authors addressed them.

As authors, we may still disagree with those who review our work, and we may choose to rebut their concerns either by addressing them within the work or through separate commentary. Others reviewers may also choose to elucidate why they find others' reviews inaccurate or prejudicial. Non-reviewers may even be able to weight in, either through our reviewing systems or through other means of public discourse. Elucidating disagreements amongst scientists gives our audience a more accurate and nuanced view of how science works than summarizing it into a single "accept" or "reject" decision.

When we are ready to publish work we have authored as scientifically peer reviewed in the elucidative model, we are required to share the reviews with audience. If we want to publish a revision before the reviews are updated in response, or if reviewers are unwilling or unable to respond, we must also share the last version(s) reviewed by each reviewer, illuminating what we have changed since they last weighed in.

Elucidative peer review doesn't reach final conclusions because science takes a long time to build consensus around when a question can finally be put to rest.  Better to avoid any pretense of finality or endorsement beyond what any individual is willing to write.

<!-- Should authors want to publish a revision that is newer than the last version a reviewer addressed, they must make that prior version available as well. This allows their audience to see what content has changed since that reviewer reviewed the work. -->

#### Elucidative peer review is better for us all

<!-- Investigate, report, conclude. -->

Elucidative peer review had advantages over prescriptive review whether we are authoring, reviewing, or consuming research.

**As reviewers**, elucidative peer review allows us to serve science without being complicit in ranking and gatekeeping. It gives our reviews a higher purpose and broader audience. It also gives lightens our reviewing workloads, as authors need not resubmit their work repeatedly in search of reviewers who consider their work interesting enough to publish—each work needs only one set of reviewers.

**As authors**, when we write work that will undergo elucidative peer review, we can write for our audience and not for reviewers, and prioritize scientific content over reviewers' subjective interests. We can expect feedback to be more constructive and timely. We need only wait for reviewers to read and respond to our work to get feedback, and we need only wait until we are satisfied that we have sufficiently addressed reviewers concerns to publish it.

Under the elucidative model, we could even submit our experimental methodology for review, using reviewers' initial feedback to improve those experimental designs, *prior* to running experiments and reporting results. We could then seek further feedback after completing the research. Currently, peer review only reveals experimental design flaws after we have spent all the resources needed to run an experiment, and often after key co-authors have graduated or started other jobs.

**As consumers** of research, we benefit from access to objective insight into the credibility of research and faster access to research, since publication is no longer gated by convincing a committee the work is interesting. We regain the autonomy to choose what interests us, no longer in the hands of peer review committees biased in favor of seniority.

As consumers of research, we might even demand more control over what interests us, such by getting express what we want to see presented at conferences so that scheduling algorithms can be optimized to *our* collective interests: allocating the largest spaces to the presentations that more of us want to see and the more intimate spaces to those with smaller audiences. (If there is not enough interest for a work to be allocated presentation space, the authors will likely have time for one-on-one discussions with those of us who want to know more about their work.)

**Those who stand to lose** by the adoption of elucidative peer review are the journals and conferences who use the veneer of scientific integrity as a pretense for employing (if the word applies for unpaid labor) scientists to maintain their prestigious brands. They are the churches and temples whose survival depends on our perverse willingness to collectively torture ourselves perpetuating an anachronistic bloodletting ritual. Most are willing because they see no alternative.

With access to elucidative peer review, and the ability to establish credibility with an audience without appealing to prestigious publications, we might see through this illusion of necessity. Without this illusion, journals and conferences could no longer extract rents for the prestige we give them through our allegiance.

The reputation of the scientific endeavor rests on our ability liberate science of its toxic vestigial functions and those that would have us perpetuate them.  As much as we may feel indebted to those publications whose acceptances helped establish our places in science's stratification system, we must recognize that these masters do not serve us, but enslave us.

<!-- #### Moving forward -->

<!-- #### Excising the vestiges of ranking

Without the pretense of protecting scientific integrity, journals are effectively awards committees that endow prestige onto work within their field or subfield. Those who like awards committees and feel they serve the community can contributing to them. The rest of us should feel no obligation.

Alas, most of us feel deep loyalty to these institutions and their brands and the validation they give us. We see criticisms of these institutions as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. Our collective loyalty and complicity with these institutions makes it hard fo us to deprogram ourselves and reckon with the harms they cause.

Some of us avoid reckoning with our complicity by placing all the blame for peer review's toxicity on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Others justify perpetuating these institutions in much the same way that homeowners justify protecting their exclusive neighborhoods. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. -->

<!-- To make elucidative peer review a reality we need institutions to run it, scientists to volunteer to review others' work, and authors to submit their work for review.

Aside from the invariant the reviewers aren't in a position to reject work, and write their reviews to bring concerns to light for the work's audience, there are many different choices that might vary in how we implement elucidative peer review.

Which institution(s) should conduct reviews? What should be the requirements for reviewers to serve? When work is submitted, how should reviewers be selected given candidates' varying expertise, objectivity, and availability for fast turn around? Should authors have to pay to have their work reviewed, should reviewers be paid, and if so how much? Should authors be anonymous through the entire review process, until a reviewer has submitted their first review, or not at all? Do reviewers need to be anonymous or pseudonymous by default even if they're not in a position to reject papers, or can they be named by default? -->



<!-- Why does everyone who has watched a procedural drama know about the criminal justice system? "In the criminal justice system, the people are represented by two separate, yet equally important, groups: the police, who investigate crime; and the district attorneys, who prosecute the offenders. These are their stories." [gavel clank, gavel clank] -->

<!-- em — , en –   …  -->
