# Rejecting *Reject* in Scientific Peer Review

Scientific peer review should *advance* science. It should promote and protect the integrity of scientific research by identifying missteps, errors, and other causes for concern that might otherwise go undetected or undisclosed. It should empower authors with others' expertise and insights to make us all better scientists and communicators.

But we use the term peer review to refer to many practices that often hinder science, not advance it.

We pit research works against each other to compete for inclusion in exclusive publications or other forms of recognition. Competitive peer review maintains the pretense of serving science and protecting scientific integrity, but the criteria used to pick winners inevitably undermines integrity<!-- []](./The-Harms-of-Exclusionary-Peer-Reivew.md)-->. Further, we cannot pick winners objectively or reliably; in the few well-designed experiments of competitive peer review, [half of the works accepted by one review were rejected by a parallel review](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/).<!-- ./Notes.md#replicability --> 

The problem is not merely that we review work competitively or using criteria beyond scientific integrity. Even if we could accept every work that met some threshold of scientific integrity, we could not do so objectively. Scientific integrity has many facets, and quantifying each dimensions, compressing them to a scalar, and again to a boolean, is lossy and perilous. Segregating works into *accepts* and *rejects* also not necessary.

The allure of the power to reject work is that it can prevent stop deeply-flawed work from misleading its audience by preventing the work from reaching that audience. However, a *reject* only censors the work from one outlet for publication. A *reject* cannot prevent authors from self-publishing that work, or seeking out a publication with less rigorous standards of peer review. When a *reject* does succeed in censoring work in which authors have taken avoidable missteps, it conceal those missteps from those who might study or learn from them, making it harder for science to learn from our collective mistakes.

Further, the power to reject has consequences beyond any one work that we might feel compelled to censor. The existence of *reject* necessitates the alternative of *accept*. Accepting work that we could have rejected will inevitably be construed as an endorsement, yet accepted works are rarely without limitations and shortcomings that many readers will overlook.

The power to reject work can also corrupt our objectivity as reviewers. The moment we start leaning toward an accept or reject outcome, [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) may lead us to favor observations that support that decision. We may be unfairly ignore observations that support the alternative, as they create dissonance with our belief that our decision is the fair objective outcome.

The surest way to avoid being corrupted by the power to reject work is to abdicate that power. The surest way to avoid the problems of compressing our evaluation of a work's integrity into a boolean is not to compress it. The surest way to ensure our that the mistakes we censor are not repeated is not to censor them. The surest way to avoid unfairly picking winners and losers is not treat scientific advancement as a competition. The product of *scientific* peer review need not, and should not, be a decision to *accept* or *reject* a work.

Peer review should instead produce a written evaluation of the work be shared with the work's authors *and* audience. To ensure the audience can benefit from those evaluations, authors who submit for this *illuminative* form of peer review should be required to share the reviews if and when they publish their work or otherwise share it with others.

In *illuminative* peer reviewer, reviewers could address their evaluations directly to the work's potential audience, to ensure that audience is not disadvantaged by lacking reviewers' expertise, or by lacking the time to read as diligently as reviewers are expected to. For deeply-flawed work, reviewers could make their concerns apparent to the work's audience, signalling a lack of credence equivalent to, or even exceeding, that which would be conveyed by a *reject* (though that audience may not see the reviews if authors choose not to publish). When reviewing particularly meticulous work, reviewers may illuminate why its credibility exceeds that which would be signalled by a mere *accept*. Conducting investigations and elucidating findings for a broad audience are essential skills for the conduct of science, and so it is reasonable to expect successful scientists invited to review others' work to have the skills needed for *illuminative* peer review.

A scientists' essential collaboration skills are also helpful for persuading authors to improve the work. Reviewers could use those skills to advocate for the value of improvements, provide constructive feedback to reduce the effort needed to make those improvements, and promise to update their reviews to acknowledge those improvements.

*Illuminative* review will still provide negative feedback, including some that authors may find unfair. While authors would be required to share that feedback, they could rebut it, encourage other reviewers to rebut it, and even invite rebuttals from additional experts if they so chose.

---

We should isolate *illuminative* scientific peer review from competitive peer review, and other forms of review that segregate works into *accepts* and *rejects*.

By employing peer review that is exclusively *illuminative*, we can ensure that reviewers write their reviews for the benefit of the work's potential audience, not to convince fellow reviewers to agree on an acceptance decision, nor to convince authors that a decision is fair.

By employing exclusively *illuminative* review, we can empower  authors to write their papers to serve their intended audience, not to court acceptance from reviewers. When writing papers for competitive review, we cannot help but try to appease reviewers' expectations. We write to justify its importance to reviewers who were assigned to read it. We may use jargon to signal that we are knowledgeable. We may also signal expertise by providing many times more references than our audience actually needs, and cite works by likely reviewers to curry favor with them. We could better serve our intended audience if writing were not an exercise in avoiding rejection.

By employing exclusively *illuminative* review, we could reduce the time authors need to wait for feedback, because most of the current delays is needed to compare competing submissions. In competitive peer review, reviewers must review a set of submissions, discuss each submission with other reviewers, and collectively determine outcomes for all submissions, before authors can receive feedback. When providing *illuminative* reviews, we could send feedback immediately after evaluating a work on its own. We could create peer review committees for time-critical topics that could respond in days instead of months. We could dramatically accelerate the pace of scientific discourse.

By opting for exclusively *illuminative* peer review, authors could save time and effort for both themselves and reviewers. Opting out of competitive review eliminates the cycle in which works are rejected by one set of reviewers only to be submitted to another set of reviewers, then another, and another. Such cycles require diligent authors to revise work for each re-submission to a new peer review committee with different members, expectations, interests, and paper formatting rule. Even if work is unchanged, each such re-submission requires a new set of reviewers to replicate the work of prior reviewers from scratch.

We could also make science more efficient by conducting *illuminative* peer review before we run experiments, so that we can improve experimental methodologies prior to incurring the costs of the experiment.  This could be especially useful for human-subjects experiments and other experiments that are expensive to re-run, or that have a potential for harm each time they are run.

Embracing *illuminative* peer review could also make science kinder and more inclusive. We could expect more constructive and positive feedback from *illuminative* peer review, at least in aggregate, because reviewers would not need to concoct reasons to justify rejecting the majority of papers. Illuminative peer review does not conflate reviewers' subjective disinterest in a work, or sense that it lacks significance, with a lack of scientific integrity. Offering peer review that is not competitive could make science more inclusive, as reviewers are naturally biased to accept work by authors similar to themselves, whether its because the work addresses problems similar to their own or whether its because the writing is in a dialect similar to their own. 

---

What *illuminative* peer review can't do is tell us if our work is valuable to others. It can't be used to determine which papers should be presented at a conference. It doesn't provide output that can quickly be fed to an algorithm that determines if the authors are worthy of a job or a lifetime achievement award. It cannot support the needs of exclusive publications that seek to curate knowledge.

Some of these functions can work without competitive peer review. Conferences, for example, could poll their attendees and allocate speaker slots for rooms of different sizes based on the number of interested attendees. We could avoid having our work competitively reviewed by submitting it for *illuminative* peer review and then offering to present it at such conferences.

Some of us will still want the prestige that comes with an exclusive publication, award, or other recognition. Competitive review can be performed subsequent to, or in place of, purely *illuminative* review.

If competitive peer review were preceded by *illuminative* review, competitive reviewers would benefit from reading the illuminative reviews.

More importantly, merely separating illuminative scientific review from competitive review would remove the pretense that competitive review serves science, when its true purpose is to distribute status to stratify researchers. Removing that pretense of service scientific integrity would help the scientific community better recognize, and reckon with, the costs of introducing zero-sum games into a cooperative endeavor meant to increase *everyone's* knowledge. It would make complicity in the toxicity of those games less of a necessity, and more of a choice.

