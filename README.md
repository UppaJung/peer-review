# What You Can Do to Fix Peer Review
<!-- # Rejecting *Reject* in Scientific Peer Review -->

<!-- , rank it, or decide whether it is worthy of recognition -->
The most objective and scientifically valuable forms of peer review do not require us to score others' research work. We can help authors improve their work, and help audiences understand it, by providing ***scoreless** peer review*.

We should be wary of assign scores to others' work because doing so amplifies our implicit biases and undermines our objectivity: we are predisposed to appreciate works that are familiar in approach, language, and style to our own work; we are predisposed to trust results that confirm our existing beliefs and hopes. Scoring requires us to evaluate how much the work deserves to be trusted, recognized, or otherwise valued, which we cannot quantify without activating our biases. These biases can then propagate from our scores to our written feedback. We may try to be objective, such as by ensuring every word we write is backed by facts, but we can only report a small fraction of the facts we observe, and facts will seem more salient when they confirm the fairness of our scoring intentions.

Despite the hazards of scoring others' work, scoring is the principal function of the most widely-available form of peer review: *publication review*—evaluating which research works are worthy of their recognition through publication by a journal or conference. Publication reviews may use different criteria and intermediate scoring systems, but they ultimately produce a score of *accept* or *reject*.[^boolean]

[^boolean]: *Accepts* and *rejects* are sometimes further subdivided into scores such as *revise and resubmit*.

The scant evidence available certainly does not suggest that assign *accepts* and *rejects* objectively. In experiments by the NeuroIPS conference in 2014 and again in 2021, half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] It's fashionable to refer to this inconsistency as ‘*randomness*’, but doing so is unscientific and harmful, perpetuating the assumption that our inconsistency is unbiased (uniformly distributed) and diminishes in impact over time and scale. The word ‘randomness’ implicitly rejects the possibility of systemic biases, such as those against underrepresented researchers and unpopular ideas, which grow worse as today's peer reviews determine who survives to become tomorrow's peer reviewers.[^selection-of-scientists]

Excising scoring from peer review cannot make us unbiased, but it does remove a key trigger likely to subconsciously activate and amplify our biases, and with it any pressure to write feedback that justifies those biased scores. Thus, removing scoring could make science more objective, equitable, and just. And, we don't need to score work to serve the two *scientific* purposes we most often use to motivate peer review: helping authors to improve their research work (author-assistive review) and helping their audience understand their work and evaluate its credibility (audience-assistive review).

#### Author-assistive review

Helping others improve research already motivates many of us to informally conduct scoreless peer review for students and colleagues. We scrutinize their work to catch errors, offer clarifications, and suggest other ways to make their work better. We ask others to scrutinize our work for the same reasons, often to give us an advantage when later submitting that work for publication review. 

*Author assistive peer review* is, alas, a service many of us provide primarily to those close to us. This social norm, and the norm of offering publication review to everyone else, makes author-assistive review far less accessible than publication review. Whereas those established in a field have access to experts willing to donate their time, others may have no choice but to get feedback by submitting their work to journals or conferences. Those already established in a field get constructive feedback quickly, whereas outsiders may wait months longer only to receive feedback written to justify a rejection outcome.[^mostly-rejections]

If we want to ensure our well-intentioned service to others does not unwittingly disadvantage underrepresented groups and other outsiders, we should each make ourselves at least as accessible for *author-assistive* peer review as we are for publication review.[^pre-experimental-review] (For how, skip down to "[What You Can Do – Right Now!](#rightnow)")

#### Audience-assistive review

Evaluating the credibility of research work is also a goal we can achieve through scoreless peer review, as using publication review for this purpose is problematic. Publications' acceptance criteria include factors such as ‘novelty’ and ‘significance’ that are not only unrelated to credibility, but subvert it by rewarding unscientific practices and chance results.[^evaluating-experiments-by-their-outcomes] Acceptances are too often misconstrued as endorsements that lead readers to overlook key limitations. Outright fraud is rarely detected by publication review and often benefits from its endorsement. And, since publication review requires scoring, under-represented groups and other outsiders will find it harder to establish that their work is credible through publication than insiders will.

*Audience-assistive peer review* is the scoreless approach to examine a work's credibility. Our goal as reviewers it to provide feedback addressed to the work's audience to help them understand the work and any issues that might impact its credibility.[^information-asymmetries] Sharing nuanced written feedback presents us the opportunity to differentiate extremely meticulous findings from those that are somewhat credible, those that are glaringly wrong, and everything in between.[^rejects-invisible-if-unpublished] We can help the audience develop the skills to identify strengths and weaknesses of research results themselves. And while audience-assistive reviewing will never be free from bias, and even credibility is subjective[^subjective-credibility], the reviews become part of science's open discourse, presenting opportunities for others to recognize and mitigate any bias that might go unaddressed if left within the shadow discourse of confidential publication review.[^open-peer-review]

Audience-assistive review has other benefits. It frees authors write to write for their audience, who will ultimately judge their work, rather than to appeal to reviewers.[^written-for-reviewers] We can provide reviews more quickly when we do not coordinate on final scoring outcomes (though reviewers may still want to later update their feedback in response to others' insights).

We all stand to benefit if research an be vetted more quickly and objectively, and we will reduce the disadvantage that under-represented groups and outsiders have in establishing the credibility of their work through publication review.

<a id="rightnow"></a>
#### What You Can Do – Right Now! – to Fix Peer Review

Peer review reform requires collective action, but it need not be a collective-action problem where we must all agree to act at once. You don't need to convince a critical mass to do it all at once, and you need not wait for a critical mass of others to agree that we should all be doing more scoreless peer review.

All you need to do for now is declare that *you* are open to performing scoreless peer review. So long as you have a publicly-visible web presence, be it a webpage, blog, or social media profile, edit it to declare that you are "available to conduct scoreless peer review". I recommend those exact words for english-language searchability, noting they are easily prefaced with conditionals like "try to be" or followed with conditionals such as "when possible".[^author-only][^fees] Then list the topic area you feel qualified and interested to review, so that those searching for reviewers can restrict their search by topic. For an example, see the [bottom of the about page on my personal blog](https://www.stuartschechter.org/about).


As with author-assistive review, we should each make ourselves as accessible for *audience-assistive* peer review as publication review. For searchability, I'd suggest the phrase "available for audience-assistive peer review" or "available for assistive peer review" to encompass both author- and audience-assistive review. Once enough experts in a field do, it will be far easier to assemble more formal assistive review panels than if organizers had to start from scratch. Make sure to be at least as helpful to those volunteering for such administrative work as you would the chairs of journals and conferences, and consider volunteering for such work yourself if you can.



Dedicating time to scoreless peer review may take time away from publication review. As more work is submitted for publication having already benefited from scoreless review, with its credibility already evaluated, less of that work may need to be rejected. Consider that work submitted to review committees with acceptance rates of 25% will require an expectation of four submissions to be published, consuming the time of four sets of reviewers. Increasing the probability of acceptance to 34% reduces the expected number of submissions by one (from 4 to 3), removing the expected review workload by an entire set of reviewers. Further, if publication review is no longer needed to establish the credibility of work, more conferences could forgo using peer reviewers to choose which works they think attendees will want to see and, instead, ask prospective attendees which works they actually want to see.

So long as those who employ scientists use recognition from journal and conference publications to make hiring and promotion decisions, many of us will feel pressured to submit our work for publication review and to serve as publication reviewers. Reforming peer review has been a challenging collective-action problem because most research works have at least one author whose career advancement requires such recognition.

If a solution is possible, it likely requires us to disentangle the competing goals we currently attribute to peer review: helping authors improve their work, establishing a work's credibility, and evaluating whether the work is worthy of recognition. At a minimum, we can broaden access to the scientific functions of peer review by broadening access to author- and audience-assistive review. But perhaps, in doing do, we can also undermine the pretense that evaluating research for recognition is in any way essential to, or even compatible with, the progress of science.
<!-- [^fashion-shows] -->


---

Stuart Schechter ([@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist)) wrote this with suggestions and feedback from  [Joseph Bonneau](https://jbonneau.com/), [Sauvik Das](https://www.hcii.cmu.edu/people/sauvik-das), [Mary L. Gray](https://marylgray.org/), [Cormac Herley](https://cormac.herley.org/), [Jon Howell](https://research.vmware.com/researchers/jon-howell), [Michael D. Smith](https://seas.harvard.edu/person/michael-smith), and [Erin Walk](https://erinwalk.org/).

Stuart is available for assistive peer review on topics including human factors in security (aka usable security) and more generally in evaluating novel experimental methodologies for human subjects experiments.

[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).

[^consistency]: [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/) followed the earlier experiment at [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS). The 2014 call for papers stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity” whereas the 2021 call for papers listed no evaluation criteria.

[^mostly-rejections]: Since journals and conferences compete to be exclusive, those submitting to the publications with the most experts can expect the highest rejection rates.

[^pre-experimental-review]: We should also offer to assist authors *before* they conduct experiments, helping them refine their hypotheses and improve their experimental designs prior to incurring the risks, costs, and time the experiments require. The current practice of reviewing and rejecting experimental designs at publication time, after the costs of conducting the experiment have been sunk, is wasteful and frustrating for all involved. Pre-experimental review and registration of experimental designs can increase the chance that null results will be published. A [2018 examination](https://psyarxiv.com/3czyt) of 127 bio-medical and psychological science registered reports (pre-registered studies) showed a 61% null result rate, as opposed to a typical 5-20% null result rate for studies published in venues that did not require pre-registration.

[^subjective-credibility]: Even *credibility* is inherently subjective. Consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the credibility of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: There are forms of publication review that open up some of their discourse, such as [open peer review](https://en.wikipedia.org/wiki/Open_peer_review), in which the reviews of accepted papers become public.

<!-- [^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so. -->

[^rejects-invisible-if-unpublished]: Further, when rejected work goes unpublished they disappear from scientific discourse, we can't learn from any mistakes that were made, and so the same mistakes and failed experiments can be re-run over and over. We bias research in favor of chance results.

[^written-for-reviewers]: Research works today are often more written for reviewers than a work's true audience, at a cost both to that audience and to authors. For example, reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the number of citations to grow out of hand. Bibliographies have evolved like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.


[^anonymity]: The corrupting influence of power to score others' work can be compounded when we exercise that power anonymously. Audience-assistive peer review can be performed anonymously, but there may be less reason to when reviews do not produce scores or outcomes.

[^selection-of-scientists]: In [The natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384), Smaldino and McElreath argue that  that publication review also favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. 

[^information-asymmetries]: Whereas publication review can reinforce knowledge asymmetries, audience-assistive feedback is designed to reduce knowledge asymmetries, reducing the knowledge gap between authors and audience.


<!-- [^fashion-shows]: Treating publication review as *scientific service* requires us to believe that what we are doing is fundamentally different from those who select which fashions to display on a runway.  -->

[^author-only]: If you are only willing to provide *author-assistive* review, and not *audience-assistive review*, I'd suggest "available to conduct author-assistive peer review."

[^fees]: Peer review as unpaid (volunteer) service has been a norm in much of the scientific community. Yet, author-assistive reviews are often performed colleagues at the same employer, or collaborators who are on the same grants, and so are often implicitly paid. We need to be understanding that reviewers may reasonably expect to be paid when reviewing industrial research and research by authors who are as well compensated, if not better compensated, than the reviewer is.