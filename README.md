# What You Can Do to Fix Peer Review
<!-- # Rejecting *Reject* in Scientific Peer Review -->

<!-- Concepts lost
   Authors incentives
   Fashion show
-->

The most valuable forms of scientific peer review do not require us to score others' work, rank it, or decide whether it is worthy of publication. We can make science more efficient, objective, and just by broadening access to scoreless peer review
<!-- beyond what we already to provide when helping students, colleagues, and friends. -->

The reason to eschew the act of scoring others' work is because it triggers and amplifies our implicit biases. We are predisposed to appreciate works that are familiar in approach, language, and style to our own work. We are predisposed to trust results that confirm our existing beliefs and hopes. Scoring requires us to quantify our sense of how much the work should be appreciated, trusted, or otherwise valued, triggering our biases. Once triggered, these biases can propagate from our scores to our written feedback even when we take great pains to be objective. For example, even if we limit our feedback to strictly factual observations, we can only report a small fraction of the facts we observe, and facts that confirm the fairness of our scoring intentions will seem more salient. 
<!-- Newcomers, underrepresented groups, and other outsiders may not only get lower scores, but worse feedback. -->

It's unclear just how much bias is present in our peer review scores and decisions, but the scant evidence available certainly does not suggest we are objective. In experiments by the NeuroIPS conference in 2014 and again in 2021, half of the works selected for acceptance by one set of peer reviewers were rejected by parallel sets of reviewers.[^consistency] It's fashionable to refer to this inconsistency as *randomness*, but doing so perpetuates the assumption that inconsistency is unbiased (impacting everyone equally) and diminishes over time. The term *randomness* dismisses the possibility of systemic bias against underrepresented researchers and unpopular ideas, which grows worse as today's peer reviews determine who survives to become tomorrow's peer reviewers. We should assume that our inconsistency may be caused by bias unless there is evidence to disprove it.

Scoreless peer review does not remove our biases, but it does remove a trigger that may activate or amplify those biases. Removing scores also removes our bias to make our written feedback match those scores, or to match the rankings or acceptance decisions that might be generated from those scores. While scoreless peer review cannot be used to decide which work to publish, it can serve the scientific purposes that are often attributed to peer review: improving research work and evaluating its credibility.

Most of us already use and conduct scoreless peer review to improve research work. We scrutinize work authored by students and colleagues at their request. We help them catch errors, we offer clarifications, and we suggest other ways to make the work better. <!-- Many of solicit, and provide, such *author-assistive* peer review on short deadlines, sometimes as little as hours, as we prepare work for submission to journals and conferences. --> Alas, social norms of offering *author-assistive* review to those we know, and publication review to everyone else, cause *author-assistive* peer review by experts to be far less broadly accessible than publication review by experts. While insiders have access to experts willing to donate their time, others may have no choice but to get feedback by submitting their work to journals or conferences. Whereas insiders get constructive feedback quickly, outsider may wait months longer only to receive feedback written to justify a rejection outcome.[^mostly-rejections]

If we want to ensure our well-intentioned service to others does not unwittingly disadvantage underrepresented groups and other outsiders, we should each make ourselves at least as available for *author-assistive* peer review as we are for publication review.[^pre-experimental-review]

[^pre-experimental-review]: We should also offer to assist authors *before* they conduct experiments, helping them refine their hypotheses and improve their experimental designs prior to incurring the risks, costs, and time the experiments require. The current practice of reviewing and rejecting experimental designs at publication time, after the costs of conducting the experiment have been sunk, is wasteful and frustrating for all involved. <!-- SD comment: there has been some preliminary evidence as well to suggest that peer-reviewing pre-registered experiments reduces publication bias towards positive findings: e.g., https://www.nature.com/articles/d41586-018-07118-1>

We can also use scoreless peer review to address the need to evaluate the credibility of research work. Using publication review for this purpose is problematic. Publications' acceptance criteria include factors such as ‘novelty’ and ‘significance’ that are not only unrelated to credibility, but subvert it by rewarding unscientific practices and chance results.[^evaluating-experiments-by-their-outcomes] Acceptances are too often misconstrued as endorsements that lead readers to overlook key limitations. Outright fraud is rarely detected by publication review, and often benefits from it. And, since publication review can be biased against outsiders, under-represented groups and other outsiders will find it harder to establish that their work is credible than insiders will.

The scoreless alternative is *audience-assistive* peer review, in which we address our feedback to the work's audience to help them understand the work and evaluate its credibility. Written feedback provides opportunity for nuance to help a work's audience learn why some results are more credible than others, and to differentiate extremely meticulous findings from those that are somewhat credible, those that are the glaringly wrong, and everything in between.[^rejects-invisible-if-unpublished]. We can not only help the audience evaluate the credibility of the work we are reviewing, but help the audience become better at identifying the strengths and weaknesses of research results on their own.

We can allow authors to request audience-assistive review immediately after making their research public, or beforehand, requiring them to release the reviews they receive along with their research.[^anonymity] Audience-assistive review can be more efficient than publication review because it does not require coordination between reviewers: they need not rank papers or coordinate on a decision, though they may end up re-evaluating their feedback after seeing others' insights. Audience-assistive reviews become part of science's open discourse and so there is more opportunity to recognize and mitigate any bias in the review process than when work is scrutinized as part of a shadow discourse.

If we want to reduce the disadvantage that under-represented groups and outsiders have in establishing the credibility of their work, we should each make ourselves at least as available for *audience-assistive* peer review as we are for publication review. Offering audience-assistive review also empowers authors who want to write their work for their audience, rather than to appeal to reviewers.[^written-for-reviewers]

As we make ourselves more available for *author-* and *audience-assistive* reviews, we may still feel obligated to perform publication review for exclusive journals and conferences and to submit our work to them. Many employers make hiring and promotion decisions by examining where our work is published, and which publications we are invited to review for, as if these were objective measures free from systemic bias.  Publication reform has been a challenging collective action problem for the scientific community because even those of us senior enough to no longer need publications collaborate with those who are obligated to publish and obligated to serve on review committees chaired by colleagues who need to demonstrate their success in doing so.
<!-- SD: redundant with previous sentence?: Reforming how we peer review work and how we evaluate each other has been a challenging collective action problem for the scientific community. -->
 
Performing audience-assistive review can help us reform science by undermining the premise that scientific research only becomes credible when it is accepted for publication. It undermines the premise that scoring research serves a scientific purpose, as opposed to serving to help rank and stratify research.

Dividing our time between author-assistive, audience-assistive, and publication review may also help us to be more selective in deciding which publication review committees deserve our time. It may lead us to question and discuss whether conferences really need our expertise to decide which work is of interest to attendees, when we might all be better served if conferences allocated presentation space by asking prospective attendees which works they were interested in seeing. It may lead us to ask if the prestige of journals is still an efficient way to draw attention to great research by authors whose work might otherwise go unnoticed. Perhaps the best way to reform publication review is to prove that the only purpose we really need it for is attention and advancement, making it easier to reckon with the question of whether it is an efficient, objective, or just tool to serve that purpose.


---

Stuart Schechter wrote this article with the inspiration and help of many (some noted [here](./Acknowledgements.md)). You can follow him at [@MildlyAggrievedScientist@mastodon.social](https://mastodon.social/@MildlyAggrievedScientist).


[^anonymity]: The corrupting influence of power to score others' work can be compounded when we exercise that power anonymously. Audience-assistive peer review can performed anonymously, but there may be less reason to when reviews do not produce scores or outcomes.

[^evaluating-experiments-by-their-outcomes]: Evaluating experiments by their outcomes encourages authors to highlight those hypotheses that yielded statistically significant results than those that didn't, hypothesize potentially significant results only after seeing data [(HARKing)](./Recommended-Readings.md#harking-hypothesizing-after-the-results-are-known), and to engage in [other irresponsible scientific practices](./Recommended-Readings.md#rein-in-the-four-horsemen-of-irreproducibility).
<!-- ALREADY in ^selection-of-scientists That poor scientific practices increase one's chance of publication has been said to cause the [natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384) and, by implication, the natural selection of bad scientists. -->
<!-- to elide details reviewers might find uninteresting (even if needed to replicate the experiment), to inflate their contributions, to dedicate more space and attention to -->
<!-- <p>Authors may be tempted to aggrandize their research to look more important. In some fields (including [mine](./Notes.md#speculation)), researchers are even pressured by reviewers to go beyond factual reporting of results to speculate about their research's impact and importance.</p> -->

[^consistency]: [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/) followed the earlier experiment at [NIPS 2014](https://nips.cc/Conferences/2014/CallForPapers) (the conference would be renamed NeurIPS). The 2014 call for papers stated “submissions will be refereed on the basis of technical quality, novelty, potential impact, and clarity” whereas the 2021 call for papers listed no evaluation criteria.

[^mostly-rejections] Since journals and conferences compete to be exclusive, those submitting to the publications with the most experts can expect the highest rejection rates.

[^subjective-integrity]: To understand why integrity is inherently subjective, consider an experiment that attempts to prove a hypothesis by rejecting a null hypothesis. The experiment does not consider or attempt to test a third hypothesis that would also lead the null hypothesis to be rejected. If a reviewer considers that third hypothesis sufficiently implausible, the third hypothesis does not impact the integrity of the experiment. If a reviewer considers the third hypothesis sufficiently plausible, they might conclude that the experiment should have been designed to disprove it as well.

[^open-peer-review]: Audience-assistive peer review is similar to [open peer review](https://en.wikipedia.org/wiki/Open_peer_review) in that reviews are published. Open peer review often a form of publication peer review and may only make requirements of publishing reviews for accepted papers.

[^social-contract]: The social contract of informative peer review requires authors to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, informing their audience of what may have changed since each reviewer last updated their review.  While the requirement to share reviews burdens authors who receive feedback they believe to be misleading or outright malicious, they can rebut that feedback themselves or ask other reviewers, or even outside experts, to do so.

[^rejects-invisible-if-unpublished]: Further, when rejected work goes unpublished they disappear from scientific discourse, we can't learn from any mistakes that were made, and so the same mistakes and failed experiments can be re-run over and over. We bias research in favor of chance results.

[^written-for-reviewers]: Research works today are often more written for reviewers than a work's true audience, at a cost both to that audience and to authors. For example, reviewers' expectations for ever-increasing numbers of cited works, and our need to exceed their expectations for citation counts when submitting work, have caused the number of citations to grow out of hands. Bibliographies have evolved like peacocks' trains: vast assemblies of plumage, most of which serve no function but to attract the admiration of those who co-evolved to prize it.

[^selection-of-scientists]: Some have even argued that natural selection favors scientists whose “poor” methods “produce the greatest number of publishable results” which leads to “increasingly high false discovery rates”. See [recommended readings](./Recommended-Readings.md/#the-natural-selection-of-bad-science) or go directly to the primary source by [Smaldino and McElreath](https://royalsocietypublishing.org/doi/10.1098/rsos.160384).



[^figure-out-where-to-put-this-footnote]: Whereas publication review can reinforce knowledge asymmetries, audience-assistive feedback is designed to reduce knowledge asymmetries, reducing the knowledge gap between authors and audience.