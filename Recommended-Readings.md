# Recommended Readings

<span id="Rein-in-the-four-horsemen-of-irreproducibility">
#### [Rein in the four horsemen of irreproducibility](https://www.nature.com/articles/d41586-019-01307-2)
Dorothy Bishop's 2019 Nature *World View* opinion, is a great resource for the history of flawed scientific practices (e.g., P-Hacking, HARKing), which I argue are encouraged and amplified by science's reliance on exclusionary peer review.
[[DOI]](https://doi.org/10.1038/d41586-019-01307-2)

<span id="The-natural-selection-of-bad-science">
#### [The natural selection of bad science](https://royalsocietypublishing.org/doi/10.1098/rsos.160384)
Paul E. Smaldino and Richard McElreath write:
>An incentive structure that rewards publication quantity will, in the absence of countervailing forces, select for methods that produce the greatest number of publishable results. This, in turn, will lead to the natural selection of poor methods and increasingly high false discovery rates.

>We term this process the natural selection of bad science to indicate that it requires no conscious strategizing nor cheating on the part of researchers. Instead, it arises from the positive selection of methods and habits that lead to publication. 

[[DOI]](https://doi.org/10.1098/rsos.160384)


<span id="HARKing-Hypothesizing-After-the-Results-are-Known">
#### [HARKing (Hypothesizing After the Results are Known)](https://journals.sagepub.com/doi/10.1207/s15327957pspr0203_4)
Norbert L. Kerr
>HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses.

In other words, in HARKing a researcher looks for differences in data that might be significant and then tests for statistical significance. If there are 20 possible differences that the researcher might look at, we can expect one in 20 to be statistically significant at the 0.05 level by chance.

[[DOI]](https://doi.org/10.1207/s15327957pspr0203_4)
