# Notes

<span id="undermining-integrity"></span>
#### Competitive peer review undermines integrity



<span id="replicability"></span>
#### On the replicability of peer review

Past attempts to test whether peer include the [The NeurIPS 2021 Consistency Experiment](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/), and its 2014 predecessor, which duplicated the review process for a fraction of papers. Per the 2021 report:

> In 2014, 49.5% of the papers accepted by the first committee were rejected by the second (with a fairly wide confidence interval as the experiment included only 116 papers).  This year, this number was 50.6%.

The experimental methodology is noteworthy for removing independent variables that might cause differences between the replica review processes. Again, in their words:

> During the assignment phase of the review process, we chose 10% of papers uniformly at random—we’ll refer to these as the “duplicated papers.”  We assigned two Area Chairs (ACs) and twice the normal number of reviewers to these papers.  With the help and guidance of the team at OpenReview, we then created a copy of each of these papers and split the ACs and reviewers at random between the two copies.  We made sure that the two ACs were assigned to two different Senior Area Chairs (SACs) so that no SAC handled both copies of the same paper.  Any newly invited reviewer for one copy was automatically added as a conflict for the other copy.  We’ll refer to the SAC, AC, and reviewers assigned to the same copy as the copy’s “committee.”  The papers’ committees were not told about the experiment and were not aware the paper had been duplicated.

This methodology assured that papers were reviewed to a single peer review committee's reviewing standards and with reviewers from the same pool of candidates (a reviewer might be reviewing a paper from replica A and a different paper for replica B).

We can expect even lower reliability under less controlled circumstances, such as when authors re-submit their work to a different peer review committee. Re-submissions means work will be evaluated by a committee with traditions and expectations, different pools of reviewers, with new prior work released in the intervening time, and with changes in what research is fashionable over the passage of time.

<span id="publishing-reviews"></span>
#### Publishing reviews

The [social contract](./Notes.md#social-contract) of *illuminative* peer review requires authors who submit their work for review to publish the reviews along with the work. If authors want to publish a revision before the reviews are updated in response to it, or if reviewers are unwilling or unable to respond to it, authors must also share the versions last reviewed by each reviewer, illuminating what may have changed since each reviewer last updated their review.

This social contract may be informal, or peer review committees may formalize it as a condition of review.

Peer review committees or others may provide a platform on which the work and/or its reviews can be updated, or this process may be manual.

Illuminative peer review can be used with anonymous or named authors, and with anonymous, pseudonymous, or named reviewers. Naming authors prior to reviewers' initial review may make them less objective. Reviewers may be more comfortable being named by default given that they don't have to fear being blamed for rejecting a paper.

<a id="open-peer-review"></a>
#### Differences with [open peer review](https://en.wikipedia.org/wiki/Open_peer_review)?

The term ‘[open peer review](https://en.wikipedia.org/wiki/Open_peer_review)’ encompasses a range of practices are released to the public, including competitive peer review where authors of rejected papers have no obligation to share reviews if their work is later published.

The goal of this article is not coin new terms, as the excess of jargon created by scientists seeking ownership of the lexicon is also among science's sins. Rather, I hoped that giving a name to *competitive* and *illuminative* peer review would be useful in convincing my fellow scientists that the former is harmful and the latter is a real and compelling alternative.


<span id="runways"></span>
#### Runways

Runways may be engineered hills, but they still seem an odd choice for a scientist to want to die on.
